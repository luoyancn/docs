\chapter{Kubernets-latest}
Kubernetes是Google开源的容器集群管理软件，可以方便的管理容器以及容器集群。
该文档适用与kubernetes1.8和kubernetes1.10版本，由于版本更迭，不适合1.10版本以上。

\section{部署的架构}
Kubernetes可以使用单节点部署，但这种模式一般只用于测试环境。而在实际的生产环境中，
必须使用多节点的方式安装。Kubernetes的最小集群需要3台节点：3个monitor，3个nodes，每个节点的角色相同。
此次部署，针对与kubernetes1.8版本及其以上，并且网络采用calico，不再使用flannel。
现在假设3台节点的ip为20.30.40.6-8，kubernetes的servcie使用的ip段为18.18.0.0/16，分配给kubernetes的
pod的ip地址段为10.1.0.0/16。

\section{通用安装}

\begin{outline}[enumerate]
  \1 配置yum源（\textattachfile{cern.repo}{\textcolor{blue}{cern.repo}}和\textattachfile{docker.repo}{\textcolor{blue}{docker.repo}}，如附件）和安装通用软件
\begin{code-block}{bash}
cp cern.repo /etc/yum.repos.d
cp docker.repo /etc/yum.repos.d
yum install etcd docker-engine-1.12.6-1.el7.centos.x86_64 \
    docker-engine-selinux-1.12.6-1.el7.centos -y
# 防止update的时候，用community版本的docker-engine替换相关的软件包
mv docker.repo docker.repo_bak
reboot
\end{code-block}

  \1 修改系统参数
\begin{code-block}{bash}
# 修改主机名
echo $HOSTNAME > /etc/hostname
# 关闭并禁用防火墙
systemctl disable firewalld;systemctl stop firewalld
# 禁用selinux
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config

# 修改内核参数
cat >> /etc/sysctl.conf <<EOF
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv4.ip_forward = 1
fs.file-max = 1000000
vm.max_map_count = 262144
fs.may_detach_mounts=1
EOF

cat>>/etc/security/limits.conf<<EOF
*               soft    nproc           65535
*               hard    nproc           65535
*               soft    nofile          655350
*               hard    nofile          655350
*               soft    core            unlimited
*               hard    core            unlimited
EOF

cat>>/etc/security/limits.d/20-nproc.conf<<EOF
*          soft    nproc     4096
root       soft    nproc     unlimited
*          soft    nproc     65535
root       soft    nproc     unlimited
EOF

# 如果操作系统使用的是3.18版本之后，最好使用overlay作为docker的存储驱动。
# 此时，需要加载overlay的内核模块
# 但特别注意的是，/var/lib/docker不能做软链接，链接到其他路径，否则会带来使用问题。
echo "overlay" > /etc/modules-load.d/overlay.conf
echo "/dev/vdb /var/lib/docker xfs defaults 0 0" >> /etc/fstab
mkfs.xfs -n ftype=1 -f /dev/vdb

# 如果操作系统使用的是<3.18的内核版本，则最好使用devicemapper作为存储驱动。
pvcreate /dev/vda3
vgcreate docker /dev/vda3
lvcreate -l +80%FREE --thinpool dockerlv docker

reboot
\end{code-block}

  \1 配置docker
\begin{code-block}{bash}
vi /usr/lib/systemd/system/docker.service
[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target

[Service]
Type=notify
# the default is not to use systemd for cgroups because the delegate issues still
# exists and systemd currently does not support the cgroup feature set required
# for containers run by docker

#Environment="HTTP_PROXY=http://192.168.8.254:18888/"

# 如果使用3.18内核版本，推荐使用overlay作为存储驱动。
# ExecStart=/usr/bin/dockerd \
#     --host=unix:///var/run/docker.sock \
#     --host=tcp://0.0.0.0:2375 \
#     --graph=/var/lib/docker \
#     --ip-forward=true \
#     --log-driver=json-file \
#     --log-level=debug \
#     --storage-driver=overlay2 \
#     --selinux-enabled=false \
#     --registry-mirror=https://docker.mirrors.ustc.edu.cn

# 如果是3.18内核以下，推荐使用devicemapper + thinlvm作为存储驱动，则启动命令需要修改一下。
ExecStart=/usr/bin/dockerd \
    --host=unix:///var/run/docker.sock \
    --host=tcp://0.0.0.0:2375 \
    --graph=/var/lib/docker \
    --ip-forward=true \
    --log-driver=json-file \
    --log-level=debug \
    --storage-driver=devicemapper \
    --storage-opt=dm.fs=xfs \
    --storage-opt=dm.basesize=10G \ # basesize一般为dockerlv这个thinlvm的1/2
    --storage-opt=dm.thinpooldev=/dev/mapper/docker-dockerlv \
    --storage-opt=dm.use_deferred_deletion=true \
    --storage-opt=dm.use_deferred_removal=true \
    --selinux-enabled=false \
    --registry-mirror=https://docker.mirrors.ustc.edu.cn

ExecReload=/bin/kill -s HUP $MAINPID
# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
#TasksMax=infinity
TimeoutStartSec=0
# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes
# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
\end{code-block}

  \1 准备对应的二进制文件

需要的二进制文件主要包含了网络（calico:\url{https://docs.projectcalico.org/v3.1/releases/}），
安全（cfssl），kubernetes以及其他插件。相关的软件（除kubernetes之外）最好都是最新版本。
\par\parshape0 \linewidth\textwidth
\begin{table}
  \rowcolors{2}{green!80!yellow!50}{green!70!yellow!40}
  \begin{tabularx}{\textwidth}{|l|X|}
  \hline
  Binary & Download URL\\ \hline
  calico & \url{https://github.com/projectcalico/cni-plugin/releases/download/v2.0.0/calico}\\
  calicoctl & \url{https://github.com/projectcalico/calicoctl/releases/download/v2.0.0/calicoctl} \\
  calico-ipam & \url{https://github.com/projectcalico/cni-plugin/releases/download/v2.0.0/calico-ipam} \\
  kube-controllers & \url{https://github.com/projectcalico/kube-controllers/releases/download/v2.0.0/kube-controllers-linux-amd64} \\
  cnitool & \url{https://github.com/containernetworking/plugins/releases/download/v0.6.0/cni-plugins-amd64-v0.6.0.tgz} \\
  loopback & \url{https://github.com/containernetworking/cni/releases/download/v0.6.0/cni-amd64-v0.6.0.tgz} \\
  kubernetes & \url{https://github.com/kubernetes} \\
  cfssl & \url{https://pkg.cfssl.org/} \\
  \hline
  \end{tabularx}
  \label{tab:Binary files}
  \caption{二进制文件}
\end{table}
将以上所有二进制文件拷贝到主机的/usr/local/bin目录下即可。
注意，该配置默认使用集群模式，需要相关节点做ssh互信。ssh互信的配置不再赘述。

  \1 设置安全证书

准备对应的目录
\begin{code-block}{bash}
mkdir -p /etc/etcd/ssl /etc/kubernetes/ssl /var/log/kubernetes/ /var/lib/kubelet/ /etc/cni/net.d
\end{code-block}
准备对应的ssl内容文件
\begin{code-block}{bash}
cat >/etc/kubernetes/ssl/ca-config.json<<EOF
{
    "signing": {
        "default": {
            "expiry": "87600h"
        },
        "profiles": {
            "kubernetes": {
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth",
                    "client auth"
                ],
                "expiry": "87600h"
            }
        }
    }
}
EOF

cat >/etc/kubernetes/ssl/ca-csr.json<<EOF
{
    "CN": "Kubernetes",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "Beijing",
            "L": "Beijing",
            "O": "k8s",
            "OU": "System"
        }
    ]
}
EOF

cat>/etc/kubernetes/ssl/admin-csr.json<<EOF
{
    "CN": "admin",
    "hosts": [],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "Beijing",
            "L": "Beijing",
            "O": "system:masters",
            "OU": "System"
        }
    ]
}
EOF

cat>/etc/kubernetes/ssl/kube-proxy-csr.json<<EOF
{
    "CN": "system:kube-proxy",
    "hosts": [],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "Beijing",
            "L": "Beijing",
            "O": "k8s",
            "OU": "System"
        }
    ]
}
EOF

# 20.30.40.0/24这种ip段的写法，在etcd当中是不支持的。
# 另外，hosts当中的ip，表示的是server端的监听地址，并不是
# client端的地址
cat>/etc/kubernetes/ssl/kubernetes-csr.json<<EOF
{
    "CN": "kubernetes",
    "hosts": [
        "18.18.0.1",
        "20.30.40.6",
        "20.30.40.7",
        "20.30.40.8",
        "10.20.30.6",
        "10.20.30.7",
        "10.20.30.8",
        "192.168.139.6",
        "192.168.139.7",
        "192.168.139.8",
        "127.0.0.1",
        "kubernetes",
        "kubernetes.default",
        "kubernetes.default.svc",
        "kubernetes.default.svc.k8s",
        "kubernetes.default.svc.k8s.zhangjl",
        "kubernetes.default.svc.k8s.zhangjl.me"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "Beijing",
            "L": "Beijing",
            "O": "k8s",
            "OU": "System"
        }
    ]
}
EOF

\end{code-block}
生成对应的证书，并分发至各个节点
\begin{code-block}{bash}
cd /etc/kubernetes/ssl
cfssl gencert -initca ca-csr.json |cfssljson -bare ca
cfssl gencert \
      -ca=ca.pem \
      -ca-key=ca-key.pem \
      -config=ca-config.json \
      -profile=kubernetes \
      admin-csr.json |cfssljson -bare admin
cfssl gencert \
      -ca=ca.pem \
      -ca-key=ca-key.pem \
      -config=ca-config.json \
      -profile=kubernetes \
      kube-proxy-csr.json |cfssljson -bare kube-proxy
cfssl gencert \
      -ca=ca.pem \
      -ca-key=ca-key.pem \
      -config=ca-config.json \
      -profile=kubernetes \
      kubernetes-csr.json |cfssljson -bare kubernetes

cp /etc/kubernetes/ssl/* /etc/etcd/ssl
# 同时，同步到其他节点
for id in {2..3};do scp /etc/kubernetes/ssl/* root@k8s$id:/etc/etcd/ssl; \
    scp /etc/kubernetes/ssl/* root@k8s$id:/etc/kubernetes/ssl;done
\end{code-block}

  \1 设置kubernetes的context
\begin{code-block}{bash}
kubectl config set-cluster kubernetes \
        --embed-certs=true \
        --certificate-authority=/etc/kubernetes/ssl/ca.pem \
        --server=https://20.30.40.6:5443
sleep 1
kubectl config set-credentials admin \
        --embed-certs=true \
        --client-certificate=/etc/kubernetes/ssl/admin.pem \
        --client-key=/etc/kubernetes/ssl/admin-key.pem
sleep 1
kubectl config set-context kubernetes \
        --cluster=kubernetes \
        --user=admin
sleep 1
kubectl config use-context kubernetes
\end{code-block}
将生成的context文件同步到其他的节点
\begin{code-block}{bash}
for id in {2..3};do scp -r /root/.kube root@k8s$id:/root/;done
\end{code-block}

  \1 设置kubelet的context
\begin{code-block}{bash}
BOOTSTRAP_TOKEN="$(head -c 16 /dev/urandom |od -An -t x |tr -d ' ')"
cat > /etc/kubernetes/token.csv <<EOF
${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,"system:kubelet-bootstrap"
EOF

kubectl config set-cluster kubernetes \
        --embed-certs=true \
        --server=https://20.30.40.6:5443 \
        --certificate-authority=/etc/kubernetes/ssl/ca.pem \
        --kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig
sleep 1
kubectl config set-credentials kubelet-bootstrap \
        --token=${BOOTSTRAP_TOKEN} \
        --kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig
sleep 1
kubectl config set-context default \
        --cluster=kubernetes \
        --user="kubelet-bootstrap" \
        --kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig
sleep 1
kubectl config use-context default \
        --kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig
sleep 1
kubectl config set-cluster kubernetes \
        --embed-certs=true \
        --server=https://20.30.40.6:5443 \
        --certificate-authority=/etc/kubernetes/ssl/ca.pem \
        --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig
sleep 1
kubectl config set-credentials kube-proxy \
        --embed-certs=true \
        --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \
        --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \
        --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig
sleep 1
kubectl config set-context default \
        --cluster=kubernetes \
        --user="kube-proxy" \
        --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig
sleep 1
kubectl config use-context default \
        --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig
\end{code-block}
将生成的context文件同步到其他的节点
\begin{code-block}{bash}
for id in {2..3};do scp /etc/kubernetes/*.kubeconfig root@kubernetes$id:/etc/kubernetes/; scp /etc/kubernetes/token.csv root@kubernetes$id:/etc/kubernetes/;done
\end{code-block}

  \1 配置etcd集群

Etcd集群最少为3台，但生产环境下，最好为5台。
\begin{code-block}{bash}
# k8s1 node, 20.30.40.6
cat>/etc/etcd/etcd.conf<<EOF
ETCD_NAME="k8s1"
ETCD_DATA_DIR="/var/lib/etcd/k8s1"
ETCD_LISTEN_PEER_URLS="https://0.0.0.0:2380"
ETCD_LISTEN_CLIENT_URLS="https://0.0.0.0:2379"
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://20.30.40.6:2380"
ETCD_INITIAL_CLUSTER="k8s1=https://20.30.40.6:2380,k8s2=https://20.30.40.7:2380,k8s3=https://20.30.40.8:2380"
ETCD_INITIAL_CLUSTER_STATE="new"
ETCD_INITIAL_CLUSTER_TOKEN="k8s-cluster-token"
# 由于etcd监听在0.0.0.0上，为了保证其他节点可以通过主机任意一块网卡都可以访问etcd集群，需要将该主机上的所有网卡地址全部都
# 填写
ETCD_ADVERTISE_CLIENT_URLS="https://20.30.40.6:2379,https://10.20.30.6:2379,https://192.168.139.6:2379"
ETCD_AUTO_COMPACTION_RETENTION="1"
ETCD_CLIENT_CERT_AUTH="true"
ETCD_CERT_FILE="/etc/etcd/ssl/kubernetes.pem"
ETCD_KEY_FILE="/etc/etcd/ssl/kubernetes-key.pem"
ETCD_TRUSTED_CA_FILE="/etc/etcd/ssl/ca.pem"
ETCD_PEER_CLIENT_CERT_AUTH="true"
ETCD_PEER_CERT_FILE="/etc/etcd/ssl/kubernetes.pem"
ETCD_PEER_KEY_FILE="/etc/etcd/ssl/kubernetes-key.pem"
ETCD_PEER_TRUSTED_CA_FILE="/etc/etcd/ssl/ca.pem"
ETCD_DEBUG="true"
ETCD_LOG_PACKAGE_LEVELS="DEBUG"
EOF

# k8s2 node, 20.30.40.7
vi /etc/etcd/etcd.conf
ETCD_NAME="k8s2"
ETCD_DATA_DIR="/var/lib/etcd/k8s2"
ETCD_LISTEN_PEER_URLS="https://0.0.0.0:2380"
ETCD_LISTEN_CLIENT_URLS="https://0.0.0.0:2379"
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://20.30.40.7:2380"
ETCD_INITIAL_CLUSTER="k8s1=https://20.30.40.6:2380,k8s2=https://20.30.40.7:2380,k8s3=https://20.30.40.8:2380"
ETCD_INITIAL_CLUSTER_STATE="new"
ETCD_INITIAL_CLUSTER_TOKEN="k8s-cluster-token"
ETCD_ADVERTISE_CLIENT_URLS="https://20.30.40.7:2379,https://10.20.30.7:2379,https://192.168.139.7:2379"
ETCD_AUTO_COMPACTION_RETENTION="1"
ETCD_CLIENT_CERT_AUTH="true"
ETCD_CERT_FILE="/etc/etcd/ssl/kubernetes.pem"
ETCD_KEY_FILE="/etc/etcd/ssl/kubernetes-key.pem"
ETCD_TRUSTED_CA_FILE="/etc/etcd/ssl/ca.pem"
ETCD_PEER_CLIENT_CERT_AUTH="true"
ETCD_PEER_CERT_FILE="/etc/etcd/ssl/kubernetes.pem"
ETCD_PEER_KEY_FILE="/etc/etcd/ssl/kubernetes-key.pem"
ETCD_PEER_TRUSTED_CA_FILE="/etc/etcd/ssl/ca.pem"
ETCD_DEBUG="true"
ETCD_LOG_PACKAGE_LEVELS="DEBUG"

# k8s3 node, 20.30.40.18
vi /etc/etcd/etcd.conf
ETCD_NAME="k8s3"
ETCD_DATA_DIR="/var/lib/etcd/k8s3"
ETCD_LISTEN_PEER_URLS="https://0.0.0.0:2380"
ETCD_LISTEN_CLIENT_URLS="https://0.0.0.0:2379"
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://20.30.40.8:2380"
ETCD_INITIAL_CLUSTER="k8s1=https://20.30.40.6:2380,k8s2=https://20.30.40.7:2380,k8s3=https://20.30.40.8:2380"
ETCD_INITIAL_CLUSTER_STATE="new"
ETCD_INITIAL_CLUSTER_TOKEN="k8s-cluster-token"
ETCD_ADVERTISE_CLIENT_URLS="https://20.30.40.8:2379,https://10.20.30.8:2379,https://192.168.139.8:2379"
ETCD_AUTO_COMPACTION_RETENTION="1"
ETCD_CLIENT_CERT_AUTH="true"
ETCD_CERT_FILE="/etc/etcd/ssl/kubernetes.pem"
ETCD_KEY_FILE="/etc/etcd/ssl/kubernetes-key.pem"
ETCD_TRUSTED_CA_FILE="/etc/etcd/ssl/ca.pem"
ETCD_PEER_CLIENT_CERT_AUTH="true"
ETCD_PEER_CERT_FILE="/etc/etcd/ssl/kubernetes.pem"
ETCD_PEER_KEY_FILE="/etc/etcd/ssl/kubernetes-key.pem"
ETCD_PEER_TRUSTED_CA_FILE="/etc/etcd/ssl/ca.pem"
ETCD_DEBUG="true"
ETCD_LOG_PACKAGE_LEVELS="DEBUG"

chown -R etcd:etcd /etc/etcd
systemctl enable etcd
systemctl start etcd

cat>>/root/.bashrc<<EOF
export DATASTORE_TYPE=kubernetes
export KUBECONFIG=/root/.kube/config
alias etcdctl='etcdctl --endpoints=https://20.30.40.6:2379,https://20.30.40.7:2379,https://20.30.40.8:2379 \
    --ca-file=/etc/etcd/ssl/ca.pem --cert-file=/etc/etcd/ssl/kubernetes.pem  \
    --key-file=/etc/etcd/ssl/kubernetes-key.pem'
EOF

# 校验etcd工作状态
source /root/.bashrc
etcdctl ls
etcdctl member list
\end{code-block}

  \1 配置docker

最好是单独给docker配置存储
\begin{code-block}{bash}
# 准备之后所需要的docker image镜像
docker pull reg.qiniu.com/quay/calico-node:v3.1.1
docker pull reg.qiniu.com/quay/calico-cni:v3.1.1
docker pull reg.qiniu.com/quay/calico-kube-controllers:v3.1.1
docker pull gcr.io/google-containers/k8s-dns-kube-dns-amd64:1.14.9
docker pull gcr.io/google-containers/k8s-dns-sidecar-amd64:1.14.9
docker pull gcr.io/google-containers/k8s-dns-dnsmasq-nanny-amd64:1.14.9
docker pull gcr.io/google-containers/heapster-amd64:v1.5.2
docker pull gcr.io/google-containers/kubernetes-dashboard-amd64:v1.8.3
docker pull gcr.io/google-containers/addon-resizer:1.8.1
docker pull gcr.io/google-containers/defaultbackend:1.4
docker pull gcr.io/google-containers/nginx-ingress-controller:0.9.0-beta.15
docker pull gcr.io/google-containers/heapster-influxdb-amd64:v1.3.3
docker pull gcr.io/google-containers/heapster-grafana-amd64:v4.4.3
docker pull gcr.io/kubernetes-helm/tiller:v2.6.0
docker pull gcr.io/google-containers/fluentd-elasticsearch:1.24
docker pull gcr.io/google-containers/kibana:v5.4.0
docker pull gcr.io/google_containers/pause:3.0
\end{code-block}

  \1 配置kubernetes controller服务

kubernetes的controller服务主要包含如下的几个部分：kube-apiserver, kube-controller-manager和kube-scheduler。
\begin{code-block}{bash}
# k8s1, 20.30.40.6
vi /usr/lib/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
User=root

ExecStart=/usr/local/bin/kube-apiserver \
    --admission-control=DefaultStorageClass,LimitRanger,NamespaceLifecycle,NodeRestriction,ResourceQuota,ServiceAccount \
    --advertise-address=20.30.40.6 \
    --allow-privileged=true \
    --apiserver-count=3 \
    --authorization-mode=RBAC,Node \
    --bind-address=20.30.40.6 \
    --client-ca-file=/etc/kubernetes/ssl/ca.pem \ # 1.10版本当中，需要删除这一行，其他不变
    --cloud-config= \
    --cloud-provider= \
    --enable-swagger-ui=true --etcd-cafile=/etc/kubernetes/ssl/ca.pem \
    --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \
    --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \
    --etcd-prefix=/kubernetes \
    --etcd-servers=https://20.30.40.6:2379,https://20.30.40.7:2379,https://20.30.40.8:2379 \
    --event-ttl=1h \
    --experimental-bootstrap-token-auth \
    --insecure-bind-address=20.30.40.6 \
    --insecure-port=7070 \
    --kubelet-https=true \
    --log-dir=/var/log/kubernetes \
    --log-flush-frequency=5s \
    --logtostderr=false \
    --runtime-config=rbac.authorization.k8s.io/v1beta1,networking.k8s.io/v1 \
    --secure-port=5443 \
    --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \
    --service-cluster-ip-range=18.18.0.0/16\
    --service-node-port-range=30000-32767 \
    --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \
    --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
    --token-auth-file=/etc/kubernetes/token.csv \
    --v=3

Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target


# k8s2, 20.30.40.7
vi /usr/lib/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
User=root

ExecStart=/usr/local/bin/kube-apiserver \
    --admission-control=DefaultStorageClass,LimitRanger,NamespaceLifecycle,NodeRestriction,ResourceQuota,ServiceAccount \
    --advertise-address=20.30.40.7 \
    --allow-privileged=true \
    --apiserver-count=3 \
    --authorization-mode=RBAC,Node \
    --bind-address=20.30.40.7 \
    --client-ca-file=/etc/kubernetes/ssl/ca.pem \
    --cloud-config= \
    --cloud-provider= \
    --enable-swagger-ui=true --etcd-cafile=/etc/kubernetes/ssl/ca.pem \
    --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \
    --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \
    --etcd-prefix=/kubernetes \
    --etcd-servers=https://20.30.40.6:2379,https://20.30.40.7:2379,https://20.30.40.8:2379 \
    --event-ttl=1h \
    --experimental-bootstrap-token-auth \
    --insecure-bind-address=20.30.40.7 \
    --insecure-port=7070 \
    --kubelet-https=true \
    --log-dir=/var/log/kubernetes \
    --log-flush-frequency=5s \
    --logtostderr=false \
    --runtime-config=rbac.authorization.k8s.io/v1beta1,networking.k8s.io/v1 \
    --secure-port=5443 \
    --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \
    --service-cluster-ip-range=18.18.0.0/16\
    --service-node-port-range=30000-32767 \
    --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \
    --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
    --token-auth-file=/etc/kubernetes/token.csv \
    --v=3

Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

# k8s3, 20.30.40.8
vi /usr/lib/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
User=root

ExecStart=/usr/local/bin/kube-apiserver \
    --admission-control=DefaultStorageClass,LimitRanger,NamespaceLifecycle,NodeRestriction,ResourceQuota,ServiceAccount \
    --advertise-address=20.30.40.8 \
    --allow-privileged=true \
    --apiserver-count=3 \
    --authorization-mode=RBAC,Node \
    --bind-address=20.30.40.8 \
    --client-ca-file=/etc/kubernetes/ssl/ca.pem \
    --cloud-config= \
    --cloud-provider= \
    --enable-swagger-ui=true --etcd-cafile=/etc/kubernetes/ssl/ca.pem \
    --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \
    --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \
    --etcd-prefix=/kubernetes \
    --etcd-servers=https://20.30.40.6:2379,https://20.30.40.7:2379,https://20.30.40.8:2379 \
    --event-ttl=1h \
    --experimental-bootstrap-token-auth \
    --insecure-bind-address=20.30.40.8 \
    --insecure-port=7070 \
    --kubelet-https=true \
    --log-dir=/var/log/kubernetes \
    --log-flush-frequency=5s \
    --logtostderr=false \
    --runtime-config=rbac.authorization.k8s.io/v1beta1,networking.k8s.io/v1 \
    --secure-port=5443 \
    --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \
    --service-cluster-ip-range=18.18.0.0/16\
    --service-node-port-range=30000-32767 \
    --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \
    --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
    --token-auth-file=/etc/kubernetes/token.csv \
    --v=3

Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

# 所有节点的kube-controller-manager.service文件全部一致
vi /usr/lib/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
User=root
ExecStart=/usr/local/bin/kube-controller-manager \
    --address=127.0.0.1 \
    --allocate-node-cidrs=false \
    --alsologtostderr \
    --cloud-config= \
    --cloud-provider= \
    --cluster-cidr=10.1.0.0/16 \
    --cluster-name=kubernetes \
    --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \
    --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \
    --configure-cloud-routes=false \
    --controller-start-interval=0 \
    --leader-elect=true \
    --leader-elect-lease-duration=15s \
    --leader-elect-renew-deadline=10s \
    --leader-elect-retry-period=2s \
    --log-dir=/var/log/kubernetes \
    --log-flush-frequency=5s \
    --logtostderr=false \
    --master=http://20.30.40.6:7070 \
    --node-cidr-mask-size=20 \
    --port=10252 \
    --root-ca-file=/etc/kubernetes/ssl/ca.pem \
    --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \
    --service-cluster-ip-range=18.18.0.0/16 \
    --v=3
Restart=on-failure
RestartSec=5
Type=simple
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

# 所有节点的kube-scheduler.service文件全部一致
vi /usr/lib/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler Plugin
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
User=root

ExecStart=/usr/local/bin/kube-scheduler \
    --address=127.0.0.1 \
    --algorithm-provider=DefaultProvider \
    --alsologtostderr \
    --leader-elect=true \
    --leader-elect-lease-duration=15s \
    --leader-elect-renew-deadline=10s \
    --leader-elect-retry-period=2s \
    --log-dir=/var/log/kubernetes \
    --log-flush-frequency=5s \
    --logtostderr=false \
    --master=http://20.30.40.6:7070 \
    --port=10251 \
    --v=3

Restart=on-failure
RestartSec=5
Type=simple
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

for id in kube-{apiserver,controller-manager,scheduler};do systemctl enable $id;systemctl start $id;done
\end{code-block}

  \1 配置kubernetes node服务

kubernetes的node服务主要包含如下的几个部分：kubelet和kube-proxy。
\begin{code-block}{bash}
# 每个节点增加calico的配置文件
cat >/etc/cni/net.d/calico-node.conf<<EOF
{
    "name": "calico-k8s-network",
    "cniVersion": "0.1.0",
    "type": "calico",
    "log_level": "info",
    "ipam": {
        "type": "calico-ipam"
    }
}
EOF
# 增加kubelet.service
# k8s1, 20.30.40.6
vi /usr/lib/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet Server
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service

[Service]
User=root

WorkingDirectory=/var/lib/kubelet

ExecStart=/usr/local/bin/kubelet \
    --address=20.30.40.6 \
    --allow-privileged=true \
    --bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig \
    --cadvisor-port=4194 \
    --cert-dir=/etc/kubernetes/ssl \
    --cni-bin-dir=/usr/local/bin \
    --cni-conf-dir=/etc/cni/net.d \
    --cloud-config= \
    --cloud-provider= \
    --cluster-dns=18.18.0.2 \         # dns的ip一定不能写错，否则会影响后续heapster的搭建和使用。
    --cluster-domain=k8s.zhangjl.me\
    --fail-swap-on=false \
    --healthz-port=10248 \
    --hostname-override= \
    --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \
    --log-dir=/var/log/kubernetes \
    --log-flush-frequency=5s \
    --logtostderr=false \
    --network-plugin=cni \
    --pod-infra-container-image=gcr.io/google_containers/pause:3.0 \
    --port=10250 \
    --read-only-port=10255 \
    --register-node=true \
    --v=3

Restart=on-failure
RestartSec=5
Type=simple
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

# k8s2, 20.30.40.7
vi /usr/lib/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet Server
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service

[Service]
User=root

WorkingDirectory=/var/lib/kubelet

ExecStart=/usr/local/bin/kubelet \
    --address=20.30.40.7 \
    --allow-privileged=true \
    --bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig \
    --cadvisor-port=4194 \
    --cert-dir=/etc/kubernetes/ssl \
    --cni-bin-dir=/usr/local/bin \
    --cni-conf-dir=/etc/cni/net.d \
    --cloud-config= \
    --cloud-provider= \
    --cluster-dns=18.18.0.2 \
    --cluster-domain=k8s.zhangjl.me\
    --fail-swap-on=false \
    --healthz-port=10248 \
    --hostname-override= \
    --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \
    --log-dir=/var/log/kubernetes \
    --log-flush-frequency=5s \
    --logtostderr=false \
    --network-plugin=cni \
    --pod-infra-container-image=gcr.io/google_containers/pause:3.0 \
    --port=10250 \
    --read-only-port=10255 \
    --register-node=true \
    --v=3

Restart=on-failure
RestartSec=5
Type=simple
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

# k8s3, 20.30.40.8
vi /usr/lib/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet Server
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service

[Service]
User=root

WorkingDirectory=/var/lib/kubelet

ExecStart=/usr/local/bin/kubelet \
    --address=20.30.40.8 \
    --allow-privileged=true \
    --bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig \
    --cadvisor-port=4194 \
    --cert-dir=/etc/kubernetes/ssl \
    --cni-bin-dir=/usr/local/bin \
    --cni-conf-dir=/etc/cni/net.d \
    --cloud-config= \
    --cloud-provider= \
    --cluster-dns=18.18.0.2 \
    --cluster-domain=k8s.zhangjl.me\
    --fail-swap-on=false \
    --healthz-port=10248 \
    --hostname-override= \
    --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \
    --log-dir=/var/log/kubernetes \
    --log-flush-frequency=5s \
    --logtostderr=false \
    --network-plugin=cni \
    --pod-infra-container-image=gcr.io/google_containers/pause:3.0 \
    --port=10250 \
    --read-only-port=10255 \
    --register-node=true \
    --v=3

Restart=on-failure
RestartSec=5
Type=simple
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

# 增加kube-proxy.service
# k8s1, 20.30.40.6
vi /usr/lib/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
User=root
ExecStart=/usr/local/bin/kube-proxy \
    --alsologtostderr=false \
    --bind-address=20.30.40.6 \
    --cluster-cidr=10.1.0.0/16 \
    --hostname-override= \
    --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \
    --log-dir=/var/log/kubernetes \
    --log-flush-frequency=5s \
    --logtostderr=false \
    --proxy-mode=iptables \
    --v=3
Restart=on-failure
RestartSec=5
Type=simple
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

# k8s2, 20.30.40.7
vi /usr/lib/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
User=root
ExecStart=/usr/local/bin/kube-proxy \
    --alsologtostderr=false \
    --bind-address=20.30.40.7 \
    --cluster-cidr=10.1.0.0/16 \
    --hostname-override= \
    --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \
    --log-dir=/var/log/kubernetes \
    --log-flush-frequency=5s \
    --logtostderr=false \
    --proxy-mode=iptables \
    --v=3
Restart=on-failure
RestartSec=5
Type=simple
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

# k8s3, 20.30.40.8
vi /usr/lib/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
User=root
ExecStart=/usr/local/bin/kube-proxy \
    --alsologtostderr=false \
    --bind-address=20.30.40.8 \
    --cluster-cidr=10.1.0.0/16 \
    --hostname-override= \
    --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \
    --log-dir=/var/log/kubernetes \
    --log-flush-frequency=5s \
    --logtostderr=false \
    --proxy-mode=iptables \
    --v=3
Restart=on-failure
RestartSec=5
Type=simple
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

for id in {1..3};do kubectl create clusterrolebinding system:node:k8s$id \
    --clusterrole=system:node --group=system:nodes \
    --user=system:node:k8s$id; sleep 2;done

kubectl create clusterrolebinding kubelet-bootstrap \
            --clusterrole=system:node-bootstrapper \
            --user=kubelet-bootstrap
for id in {kubelet,kube-proxy};do systemctl enable $id;systemctl start $id;done

kubectl get certificatesigningrequest | awk '/Pending/{print $1}' |xargs -i kubectl certificate approve {}
kubectl label namespace kube-system ns-name=kube-system --overwrite
for id in {1..3};do kubectl label node k8s$id node-role.kubernetes.io/master=k8s$id \
    --overwrite;kubectl label node k8s$id node-role.kubernetes.io/node=k8s$id \
    --overwrite;done

\end{code-block}

  \1 配置Calico服务

在该文档中，不再使用flannel作为容器集群的网络方案，而是使用calico。calico大部分都是容器，因此需要借助k8s本身
来部署calico。需要使用到的k8s的部署yaml模板文件主要是\textattachfile{calico.yaml}{\textcolor{blue}{calico.yaml}}.
\begin{code-block}{bash}
# 修改模板文件内容
sed -i -r \
    -e "s|__CALICO_NODE_IMAGE__|reg.qiniu.com/quay/calico-node:v3.1.1|" \
    -e "s|__CALICO_CNI_IMAGE__|reg.qiniu.com/quay/calico-cni:v3.1.1|" \
    -e "s|__CALICO_KUBE_CONTROLLERS_IMAGE__|reg.qiniu.com/quay/calico-kube-controllers:v3.1.1|" \
    -e "s|__CALICO_IPV4POOL_CIDR__|10.1.0.0/16|" \
    -e "s|__CNI_BIN_DIR__|/usr/local/bin|" \
    -e "s|__CNI_CONF_DIR__|/etc/cni/net.d|" \
    -e "s|__DESTINATION__|20.30.40.1|" \
    -e "s|^(  etcd_endpoints: ).*|\1\"https://20.30.40.6:2379,https://20.30.40.7:2379,https://20.30.40.8:2379\"|" \
    /opt/calico/calico.yaml

ETCD_CA_FILE_BASE64=$(base64 "/etc/kubernetes/ssl/ca.pem" |tr -d '\n')
ETCD_CERT_FILE_BASE64=$(base64 "/etc/kubernetes/ssl/kubernetes.pem" |tr -d '\n')
ETCD_KEY_FILE_BASE64=$(base64 "/etc/kubernetes/ssl/kubernetes-key.pem" |tr -d '\n')
sed -i -r \
    -e "s|^  #? *etcd-ca: .*|  etcd-ca: ${ETCD_CA_FILE_BASE64}|" \
    -e "s|^  #? *etcd-cert: .*|  etcd-cert: ${ETCD_CERT_FILE_BASE64}|" \
    -e "s|^  #? *etcd-key: .*|  etcd-key: ${ETCD_KEY_FILE_BASE64}|" \
    -e "s|^(  etcd_ca: ).*|\1\"/calico-secrets/etcd-ca\"|" \
    -e "s|^(  etcd_cert: ).*|\1\"/calico-secrets/etcd-cert\"|" \
    -e "s|^(  etcd_key: ).*|\1\"/calico-secrets/etcd-key\"|" \
    /opt/calico/calico.yaml

# 创建calico需要的容器
kubectl create -f /opt/calico

# 备份calico的配置文件
for id in k8s{1..3};do ssh $id mv /etc/cni/net.d/calico-node.conf /etc/cni/net.d/calico-node.conf_bak;done

calicoctl node status
\end{code-block}

\1 安装kubedns

在该文档中，由于kubedns采用容器的方式进行部署，因此需要在calico部署完成之后再进行，否则，会因为pod无法分配到ip，导致kubedns部署失败。
需要使用到的kubedns的部署yaml模板文件主要是\textattachfile{kubedns.yaml}{\textcolor{blue}{kubedns.yaml}}.
\begin{code-block}{bash}
sed -i -r \
    -e "s|__KUBE_DNS_DOMAIN__|k8s.zhangjl.me|" \
    -e "s|__KUBE_DNS_VERSION__|1.14.9|" \
    -e "s|__KUBE_DNS_IMAGE__|gcr.io/google-containers/k8s-dns-kube-dns-amd64:1.14.9|" \
    -e "s|__KUBE_DNSMASAQ_IMAGE__|gcr.io/google-containers/k8s-dns-dnsmasq-nanny-amd64:1.14.9|" \
    -e "s|__KUBE_DNS_SIDECAR_IMAGE__|gcr.io/google-containers/k8s-dns-sidecar-amd64:1.14.9|" \
    -e "s|__KUBE_CLUSTER_DNS_SVC_IP__|18.18.0.2|" \
    kubedns.yaml

kubectl create -f kubedns.yaml
\end{code-block}

\1 安装dashboard

在该文档中，dashboard采用容器的方式进行部署，需要使用到的dashboard的部署yaml模板文件主要是\textattachfile{dashboard.yaml}{\textcolor{blue}{dashboard.yaml}}.
\begin{code-block}{bash}
    sed -i -r \
    -e "s|__KUBE_INSECURE_APISERVER__|http://20.30.40.6:7070|" \
    -e "s|__KUBE_DASHBOARD_VERSION__|v1.8.3|" \
    -e "s|__KUBE_DASHBOARD_IMAGE__|gcr.io/google-containers/kubernetes-dashboard-amd64:v1.8.3|" \
    -e "s|__KUBE_DASHBOARD_PORT__|30083|" \
    -e "s|__KUBE_DNS_DOMAIN__|k8s.zhangjl.me|" \
    dashboard.yaml

kubectl create -f dashboard.yaml
\end{code-block}

\1 安装heapster

Heapster是kubernetes默认的监控插件。该服务也是以容器的方式进行部署。需要使用到的yaml模板文件主要是
\textattachfile{influxdb-grafana.yaml}{\textcolor{blue}{influxdb-grafana.yaml}}，
\textattachfile{heapster.yaml}{\textcolor{blue}{heapster.yaml}}。
\begin{code-block}{bash}
sed -i -r \
    -e "s|__KUBE_HEAPSTER_GRAFANA_VERSION__|v4.4.3|" \
    -e "s|__KUBE_HEAPSTER_GRAFANA_IMAGE__|gcr.io/google-containers/heapster-grafana-amd64:v4.4.3|" \
    -e "s|__KUBE_HEAPSTER_INFLUXDB_VERSION__|v1.3.3|" \
    -e "s|__KUBE_HEAPSTER_INFLUXDB_IMAGE__|gcr.io/google-containers/heapster-influxdb-amd64:v1.3.3|" \
    -e "s|__KUBE_HEAPSTER_VERSION__|v1.5.2|" \
    -e "s|__KUBE_HEAPSTER_IMAGE__|gcr.io/google-containers/heapster-amd64:v1.5.2|" \
    -e "s|__KUBE_ADDON_RESIZER_IMAGE__|gcr.io/google-containers/addon-resizer:1.8.1|" \
    -e "s|__KUBE_LOCAL_VOLUME_MNT_DIR__|/var/lib/k8s|" \
    -e "s|__nanny_memory__|128Mi|" \
    -e "s|__base_metrics_cpu__|80m|" \
    -e "s|__base_metrics_memory__|140Mi|" \
    -e "s|__metrics_memory_per_node__|4|" \
    -e "s|__base_eventer_memory__|190Mi|" \
    -e "s|__eventer_memory_per_node__|500|" \
    influxdb-grafana.yaml

sed -i -r \
    -e "s|__KUBE_HEAPSTER_GRAFANA_VERSION__|v4.4.3|" \
    -e "s|__KUBE_HEAPSTER_GRAFANA_IMAGE__|gcr.io/google-containers/heapster-grafana-amd64:v4.4.3|" \
    -e "s|__KUBE_HEAPSTER_INFLUXDB_VERSION__|v1.3.3|" \
    -e "s|__KUBE_HEAPSTER_INFLUXDB_IMAGE__|gcr.io/google-containers/heapster-influxdb-amd64:v1.3.3|" \
    -e "s|__KUBE_HEAPSTER_VERSION__|v1.5.2|" \
    -e "s|__KUBE_HEAPSTER_IMAGE__|gcr.io/google-containers/heapster-amd64:v1.5.2|" \
    -e "s|__KUBE_ADDON_RESIZER_IMAGE__|gcr.io/google-containers/addon-resizer:1.8.1|" \
    -e "s|__KUBE_LOCAL_VOLUME_MNT_DIR__|/var/lib/k8s|" \
    -e "s|__nanny_memory__|128Mi|" \
    -e "s|__base_metrics_cpu__|80m|" \
    -e "s|__base_metrics_memory__|140Mi|" \
    -e "s|__metrics_memory_per_node__|4|" \
    -e "s|__base_eventer_memory__|190Mi|" \
    -e "s|__eventer_memory_per_node__|500|" \
    heapster.yaml

kubectl apply -f influxdb-grafana.yaml
sleep 20
kubectl apply -f heapster.yaml
\end{code-block}
注意，如果heapster对应的pod运行不正常，请仔细检查kubedns服务以及dns的ip是否为18.18.0.2。

\1 安装ingress

Ingress同样使用容器的方式进行部署，部署过程中需要使用的yaml模板文件主要是
\textattachfile{configmap.yaml}{\textcolor{blue}{configmap.yaml}}，
\textattachfile{default-backend.yaml}{\textcolor{blue}{default-backend.yaml}}
和\textattachfile{nginx-ingress-controller.yaml}{\textcolor{blue}{nginx-ingress-controller.yaml}}
需要注意，ingress-controller已经不再放到gcr上了，同步的更新地址为\url{https://github.com/kubernetes/ingress-nginx/releases}。
\begin{code-block}{bash}
sed -i -r \
    -e "s|__KUBE_NGINX_INGRESS_CONTROLLER_IMAGE__|quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.12.0" \
    -e "s|__KUBE_NGINX_INGRESS_CONTROLLER_VERSION__|0.12.0|" \
    -e "s|__KUBE_INSECURE_APISERVER__|http://20.30.40.6:7070|" \
    -e "s|__KUBE_INGRESS_PORT__|30082|" \
    nginx-ingress-controller.yaml

sed -i -r \
    -e "s|__KUBE_DEFAULTBACKEND_IMAGE__|gcr.io/google-containers/defaultbackend:1.4|" \
    -e "s|__KUBE_DEFAULTBACKEND_VERSION__|1.4|" \
    default-backend.yaml

kubectl create -f /opt/ingress
\end{code-block}

\1 设置自动补全
\begin{code-block}{bash}
yum install bash-completion -y
kubectl completion bash > /etc/bash_completion.d/kubectl-completion
source /etc/bash_completion.d/kubectl-completion
\end{code-block}

至此，基本的一个kubernetes平台已经搭建完毕，可以正常使用了。
\end{outline}
