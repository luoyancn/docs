\chapter{Mitaka部署安装}

\section{配置操作系统}
\label{section:system_configuration}
\begin{outline}[enumerate]

\1 关闭selinux
\begin{code-in-enumerate}{bash}
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
\end{code-in-enumerate}

\1 修改操作系统连接数
\begin{code-in-enumerate}{bash}
cat>>/etc/security/limits.conf<<EOF
*               soft    nproc           65535
*               hard    nproc           65535
*               soft    nofile          655350
*               hard    nofile          655350
*               soft    core            unlimited
*               hard    core            unlimited
EOF

cat>>/etc/security/limits.d/20-nproc.conf<<EOF
*          soft    nproc     65535
root       soft    nproc     unlimited
EOF
\end{code-in-enumerate}

\1 修改内核参数
\begin{code-in-enumerate}{bash}
# 网络节点
cat /etc/sysctl.conf
net.ipv4.ip_forward=1
net.ipv4.conf.all.rp_filter=0
net.ipv4.conf.default.rp_filter=0

# 计算节点
cat /etc/sysctl.conf
net.ipv4.conf.all.rp_filter=0
net.ipv4.conf.default.rp_filter=0
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
\end{code-in-enumerate}

\1 修改网卡配置
\begin{code-in-enumerate}{bash}
yum erase NetworkManager* firewalld -y
systemctl enable network
yum install openvswitch
systemctl enable openvswitch
systemctl start openvswitch
ovs-vsctl add-br br-ex

cat >/etc/sysconfig/network-scripts/ifcfg-eth0<<EOF
TYPE=Ethernet
BOOTPROTO=static
DEFROUTE=yes
PEERDNS=no
PEERROUTES=no
IPV4_FAILURE_FATAL=no
IPV6INIT=no
NAME=eth0
ONBOOT=yes
DEVICE=eth0
DEVICETYPE=ovs
OVS_BRIDGE=br-ex
TYPE=OVSPort
EOF

cat >/etc/sysconfig/network-scripts/ifcfg-br-ex<<EOF
BOOTPROTO=static
DEFROUTE=yes
PEERDNS=no
PEERROUTES=no
IPV4_FAILURE_FATAL=no
IPV6INIT=no
NAME=br-ex
ONBOOT=yes
DEVICE=br-ex
IPADDR=10.1.1.4
NETMASK=255.255.255.0
GATEWAY=10.1.1.1
DEVICETYPE=ovs
TYPE=OVSBridge
EOF

cat >/etc/sysconfig/network-scripts/ifcfg-eth1<<EOF
TYPE="Ethernet"
BOOTPROTO="static"
DEFROUTE="no"
PEERDNS="no"
PEERROUTES="no"
IPV4_FAILURE_FATAL="no"
IPV6INIT="no"
NAME="eth1"
ONBOOT="yes"
DEVICE=eth1
IPADDR=10.2.2.4
NETMASK=255.255.255.0
EOF

cat >/etc/sysconfig/network-scripts/ifcfg-eth2<<EOF
TYPE="Ethernet"
BOOTPROTO="static"
DEFROUTE="no"
PEERDNS="no"
PEERROUTES="no"
IPV4_FAILURE_FATAL="no"
IPV6INIT="no"
NAME="eth2"
ONBOOT="yes"
DEVICE=eth2
IPADDR=10.3.3.4
NETMASK=255.255.255.0
EOF

reboot
\end{code-in-enumerate}

\end{outline}

\section{安装Ceph}
\label{section:ceph_configuration}
\begin{outline}[enumerate]

\1 安装ceph软件
\begin{code-in-enumerate}{bash}
yum install https://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-1-1.el7.noarch.rpm -y
yum install ceph -y
\end{code-in-enumerate}

\1 controller初始配置
\begin{code-in-enumerate}{bash}
cat >/etc/ceph/ceph.conf<<EOF
[global]
fsid = a7f64266-0894-4f1e-a635-d0aeaca0e993
public network = 10.2.2.0/24
cluster network = 10.2.2.0/24
auth cluster required = cephx
auth service required = cephx
auth client required = cephx
mon initial members = controller, compute1, compute2
mon host = 10.2.2.4, 10.2.2.5, 10.2.2.6
osd pool default size = 3
osd pool default min size = 1
osd pool default pg num = 333
osd pool default pgp num = 333
osd crush chooseleaf type = 1
ms_type=async
debug_lockdep = 0/0
debug_context = 0/0
debug_crush = 0/0
debug_buffer = 0/0
debug_timer = 0/0
debug_filer = 0/0
debug_objecter = 0/0
debug_rados = 0/0
debug_rbd = 0/0
debug_journaler = 0/0
debug_objectcatcher = 0/0
debug_client = 0/0
debug_osd = 0/0
debug_optracker = 0/0
debug_objclass = 0/0
debug_filestore = 0/0
debug_journal = 0/0
debug_ms = 0/0
debug_monc = 0/0
debug_tp = 0/0
debug_auth = 0/0
debug_finisher = 0/0
debug_heartbeatmap = 0/0
debug_perfcounter = 0/0
debug_asok = 0/0
debug_throttle = 0/0
debug_mon = 0/0
debug_paxos = 0/0
debug_rgw = 0/0
EOF

ceph-authtool --create-keyring /etc/ceph/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *'
ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin \
    --set-uid=0 --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow'
ceph-authtool /etc/ceph/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring
monmaptool --create --add controller 10.2.2.4 --add compute1 10.2.2.5 --add compute2 10.2.2.6 \
    --fsid a7f64266-0894-4f1e-a635-d0aeaca0e993 /etc/ceph/monmap
scp -r /etc/ceph/* root@compute1:/etc/ceph
scp -r /etc/ceph/* root@compute2:/etc/ceph
\end{code-in-enumerate}

\1 ceph初始化
\begin{code-in-enumerate}{bash}
export HOSTNAME=`hostname`
ceph-mon --mkfs -i $HOSTNAME --monmap /etc/ceph/monmap --keyring /etc/ceph/ceph.mon.keyring
touch /var/lib/ceph/mon/ceph-$HOSTNAME/done
systemctl enable ceph-mon@$HOSTNAME
systemctl start ceph-mon@$HOSTNAM

# osd的安装最好是依次进行
# controller
ceph-disk prepare /dev/vdb
ceph-disk activate /dev/vdb1

# compute1
ceph-disk prepare /dev/vdb
ceph-disk activate /dev/vdb1

# compute2
ceph-disk prepare /dev/vdb
ceph-disk activate /dev/vdb1
\end{code-in-enumerate}

\1 ceph配置openstack资源池
\begin{code-in-enumerate}{bash}
ceph osd pool create images 3 3
ceph osd pool create volumes 3 3

# 创建openstack需要的用户
ceph auth get-or-create client.awcloud mon 'allow r' \
    osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes'
ceph auth get-or-create client.glance mon 'allow r' \
    osd 'allow class-read object_prefix rbd_children, allow rwx pool=images'

# 生成需要的keyring文件
ceph auth get-or-create client.awcloud | tee /etc/ceph/ceph.client.awcloud.keyring
ceph auth get-or-create client.glance | tee /etc/ceph/ceph.client.glance.keyring

# 分发到其他的节点
scp /etc/ceph/ceph.client.awcloud.keyring root@compute1:/etc/ceph
scp /etc/ceph/ceph.client.glance.keyring root@compute1:/etc/ceph

scp /etc/ceph/ceph.client.awcloud.keyring root@compute2:/etc/ceph
scp /etc/ceph/ceph.client.glance.keyring root@compute2:/etc/ceph
\end{code-in-enumerate}

\end{outline}

\section{Rabbitmq安装配置}
\label{section:rabbitmq_configuration}
\begin{code-block}{bash}
yum install rabbitmq-server -y

systemctl enable rabbitmq-server
systemctl start rabbitmq-server

rabbitmq-plugins enable rabbitmq_management
mv /etc/rabbitmq/rabbitmq.config /etc/rabbitmq/rabbitmq.config_bak
cat >/etc/rabbitmq/rabbitmq.config<<EOF
[
{rabbit, [{loopback_users, []}]}
].
EOF
systemctl restart rabbitmq-server
\end{code-block}

\section{Keystone安装配置}
\begin{code-block}{bash}
# 安装软件包
yum install openstack-keystone openstack-utils httpd mod_wsgi python-keystone \
    python-openstackclient -y

mysql
CREATE DATABASE keystone;
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY 'keystone';
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'keystone';
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'controller' IDENTIFIED BY 'keystone';

# 修改配置文件
openstack-config --set /etc/keystone/keystone.conf database connection \
    mysql+pymysql://keystone:keystone@controller/keystone
openstack-config --set /etc/keystone/keystone.conf token provider fernet
openstack-config --set /etc/keystone/keystone.conf DEFAULT admin_token ADMIN_TOKEN
openstack-config --set /etc/keystone/keystone.conf DEFAULT debug true

# 同步数据库
keystone-manage db_sync
# 创建加密和认证证书
keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone

# Mitaka版本不需要这一步
# keystone-manage credential_setup --keystone-user keystone --keystone-group keystone

# 修改文件所有者
chown -R keystone:keystone /etc/keystone /var/lib/keystone /var/log/keystone

# 创建http启动的wsgi文件
cp /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/wsgi-keystone.conf

# 使用httpd 启动keystone
systemctl enable httpd
systemctl start httpd

# 设置初始的环境变量
export OS_TOKEN=ADMIN_TOKEN
export OS_URL=http://controller:35357/v3
export OS_IDENTITY_API_VERSION=3

# 创建服务
openstack service create --name keystone --description "OpenStack Identity" identity
openstack endpoint create --region wuhan identity public http://controller:5000/v3
openstack endpoint create --region wuhan identity internal http://controller:5000/v3
openstack endpoint create --region wuhan identity admin http://controller:35357/v3

# 创建角色
openstack role create admin
openstack role create service
openstack role create domain_admin
openstack role create project_admin
openstack role create guest
openstack role create member

# 创建default domain，并重启服务
export DEFAULT_DOMAIN_ID=`openstack domain create default | grep -w id | awk '{print $4}'`
openstack-config --set /etc/keystone/keystone.conf identity default_domain_id \
    $DEFAULT_DOMAIN_ID
unset DEFAULT_DOMAIN_ID
systemctl restart httpd

# 创建project和用户，并对用户赋权
openstack project create --domain default --description "Admin Project" admin
openstack project create --domain default --description "Service Project" service

openstack user create --domain default --project admin --project-domain default \
    --password admin admin
openstack role add --domain default --user admin --project-domain default \
    --user-domain default admin --inherited
openstack role add --project admin --user admin --project-domain default \
    --user-domain default admin

# 取消环境变量
unset OS_TOKEN OS_URL
unset OS_URL
unset OS_IDENTITY_API_VERSION

# 创建环境变量文件
cat >/root/keystone_admin_v3<<EOF
export OS_PROJECT_DOMAIN_NAME=default
export OS_USER_DOMAIN_NAME=default
export OS_PROJECT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=admin
export OS_AUTH_URL=http://controller:35357/v3
export OS_IDENTITY_API_VERSION=3
export OS_IMAGE_API_VERSION=2
export OS_ENDPOINT_TYPE=internal
export OS_INTERFACE=internal
export PS1='[\u@\h \W(keystone_admin_v3)]$ '
EOF

# 使用环境变量文件
source /root/keystone_admin_v3

# 校验环境是否正常
openstack token issue
\end{code-block}

\section{Glance安装配置}
\begin{code-block}{bash}
yum install openstack-glance python-glance openstack-utils -y

# 设置数据库
mysql
CREATE DATABASE glance;
GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' IDENTIFIED BY 'glance';
GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' IDENTIFIED BY 'glance';
GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'controller' IDENTIFIED BY 'glance';

# 添加glance服务
openstack service create --name glance  image
openstack endpoint create --region wuhan image public http://controller:9292
openstack endpoint create --region wuhan image internal http://controller:9292
openstack endpoint create --region wuhan image admin http://controller:9292

# 添加glance用户
openstack user create --domain default --project service --project-domain default \
    --password glance glance
openstack role add --project service --user glance --project-domain default \
    --user-domain default admin

# 修改配置文件
openstack-config --set /etc/glance/glance-api.conf DEFAULT show_image_direct_url True
openstack-config --set /etc/glance/glance-api.conf DEFAULT show_multiple_locations True
openstack-config --set /etc/glance/glance-api.conf DEFAULT workers 2
openstack-config --set /etc/glance/glance-api.conf DEFAULT debug True
openstack-config --set /etc/glance/glance-api.conf database connection \
    mysql+pymysql://glance:glance@controller/glance
openstack-config --set /etc/glance/glance-api.conf glance_store stores rbd
openstack-config --set /etc/glance/glance-api.conf glance_store rbd_store_pool images
openstack-config --set /etc/glance/glance-api.conf glance_store rbd_store_user glance
openstack-config --set /etc/glance/glance-api.conf glance_store rbd_store_ceph_conf \
    /etc/ceph/ceph.conf
openstack-config --set /etc/glance/glance-api.conf glance_store rbd_store_chunk_size 8
openstack-config --set /etc/glance/glance-api.conf glance_store default_store rbd
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_uri http://controller:5000
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_url http://controller:35357
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_type password
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_domain_name default
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken user_domain_name default
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_name service
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken username glance
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken password glance
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken service_token_roles_required true
openstack-config --set /etc/glance/glance-api.conf paste_deploy flavor keystone

openstack-config --set /etc/glance/glance-registry.conf DEFAULT workers 2
openstack-config --set /etc/glance/glance-registry.conf database connection \
    mysql+pymysql://glance:glance@controller/glance
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_uri http://controller:5000
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_url http://controller:35357
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_type password
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_domain_name default
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken user_domain_name default
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_name service
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken username glance
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken password glance
openstack-config --set /etc/glance/glance-registry.conf paste_deploy flavor keystone

# 同步glance数据库
glance-manage db sync

# 修改文件所有者
chown -R glance:glance /etc/glance/ /var/lib/glance/ /var/log/glance/

# 启动glance服务
for id in openstack-glance-{api,registry};do systemctl enable $id;systemctl start $id;done

# 校验glance服务
glance image-list

# 上传glance镜像
wget http://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img
qemu-img convert -O raw cirros-0.3.5-x86_64-disk.img cirros-0.3.5-x86_64-disk-raw.img
glance image-create --name cirros --disk-format raw --container-format bare \
    --visibility public --file cirros-0.3.5-x86_64-disk-raw.img
\end{code-block}

\section{Cinder安装配置}
\begin{code-block}{bash}
yum install openstack-cinder python-cinder openstack-utils -y

# 设置数据库
mysql
CREATE DATABASE cinder;
GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'localhost' IDENTIFIED BY 'cinder';
GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'%' IDENTIFIED BY 'cinder';
GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'controller' IDENTIFIED BY 'cinder';

# 添加cinder服务
openstack service create --name cinderv2 volumev2
openstack endpoint create --region wuhan   volumev2 admin http://controller:8776/v2/%\(tenant_id\)s
openstack endpoint create --region wuhan   volumev2 public http://controller:8776/v2/%\(tenant_id\)s
openstack endpoint create --region wuhan   volumev2 internal http://controller:8776/v2/%\(tenant_id\)s

# 添加cinder用户
openstack user create --domain default --project service --project-domain default --password cinder cinder
openstack role add --project service --user cinder --project-domain default --user-domain default admin

# 修改配置文件
openstack-config --set /etc/cinder/cinder.conf DEFAULT transport_url rabbit://guest:guest@controller
openstack-config --set /etc/cinder/cinder.conf DEFAULT glance_api_version 2
openstack-config --set /etc/cinder/cinder.conf DEFAULT glance_api_servers http://controller:9292
openstack-config --set /etc/cinder/cinder.conf DEFAULT default_volume_type ceph
openstack-config --set /etc/cinder/cinder.conf DEFAULT my_ip 10.2.2.4
openstack-config --set /etc/cinder/cinder.conf DEFAULT enabled_backends ceph
openstack-config --set /etc/cinder/cinder.conf DEFAULT auth_strategy keystone

openstack-config --set /etc/cinder/cinder.conf database connection \
    mysql+pymysql://cinder:cinder@controller/cinder

openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_uri http://controller:5000
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_url http://controller:35357
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_type password
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken project_domain_name default
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken user_domain_name default
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken project_name service
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken username cinder
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken password cinder
openstack-config --set /etc/cinder/cinder.conf oslo_concurrency lock_path /var/lib/cinder/tmp

openstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_host controller

openstack-config --set /etc/cinder/cinder.conf ceph volume_backend_name ceph
openstack-config --set /etc/cinder/cinder.conf ceph rbd_secret_uuid a7f64266-0894-4f1e-a635-d0aeaca0e993
openstack-config --set /etc/cinder/cinder.conf ceph rbd_user awcloud
openstack-config --set /etc/cinder/cinder.conf ceph rbd_pool volumes
openstack-config --set /etc/cinder/cinder.conf ceph rbd_ceph_conf /etc/ceph/ceph.conf
openstack-config --set /etc/cinder/cinder.conf ceph recalculate_allocated_capacity=True
openstack-config --set /etc/cinder/cinder.conf ceph recalculate_allocated_capacity True
openstack-config --set /etc/cinder/cinder.conf ceph rbd_max_clone_depth 3
openstack-config --set /etc/cinder/cinder.conf ceph volume_backend_name ceph
openstack-config --set /etc/cinder/cinder.conf ceph rados_connect_timeout -1
openstack-config --set /etc/cinder/cinder.conf ceph volume_driver cinder.volume.drivers.rbd.RBDDriver
openstack-config --set /etc/cinder/cinder.conf ceph rbd_flatten_volume_from_snapshot False
openstack-config --set /etc/cinder/cinder.conf ceph rbd_store_chunk_size 4
openstack-config --set /etc/cinder/cinder.conf ceph backend_host volumes

# 同步数据库
cinder-manage db sync

# 设置文件所有者
chown -R cinder:cinder /etc/cinder/ /var/lib/cinder/ /var/log/cinder/

# 启动cinder服务
for id in openstack-cinder-{api,scheduler,volume};do systemctl enable $id;systemctl start $id;done

# 校验cinder服务
cinder list

# 创建cinder volume
cinder create 1

# 校验cinder与glance的交互
export image_id=`glance image-list | grep cirros | awk '{print $2}'`
cinder create 1 --image-id $image_id
\end{code-block}

\section{Neutron安装配置}
\begin{outline}[enumerate]

\1 通用安装-所有节点
\begin{code-in-enumerate}{bash}
yum install openstack-neutron python-neutron openstack-neutron-ml2 \
    openstack-neutron-openvswitch ebtables net-tools openstack-utils  -y
# 修改neutron.conf配置项
openstack-config --set /etc/neutron/neutron.conf DEFAULT api_workers 4
openstack-config --set /etc/neutron/neutron.conf DEFAULT core_plugin ml2
openstack-config --set /etc/neutron/neutron.conf DEFAULT service_plugins router
openstack-config --set /etc/neutron/neutron.conf DEFAULT transport_url rabbit://guest:guest@controller
openstack-config --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystone
openstack-config --set /etc/neutron/neutron.conf DEFAULT router_distributed true
openstack-config --set /etc/neutron/neutron.conf DEFAULT l3_ha false
openstack-config --set /etc/neutron/neutron.conf DEFAULT max_l3_agents_per_router 0
# 根据网络节点个数配置
openstack-config --set /etc/neutron/neutron.conf DEFAULT min_l3_agents_per_router 1
openstack-config --set /etc/neutron/neutron.conf database connection \
    mysql+pymysql://neutron:neutron@controller/neutron
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_uri http://controller:5000
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_url http://controller:35357
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_type password
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_domain_name default
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken user_domain_name default
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_name service
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken username neutron
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken password neutron
openstack-config --set /etc/neutron/neutron.conf nova auth_url http://controller:35357
openstack-config --set /etc/neutron/neutron.conf nova auth_type password
openstack-config --set /etc/neutron/neutron.conf nova project_domain_name default
openstack-config --set /etc/neutron/neutron.conf nova user_domain_name default
openstack-config --set /etc/neutron/neutron.conf nova region_name wuhan
openstack-config --set /etc/neutron/neutron.conf nova project_name service
openstack-config --set /etc/neutron/neutron.conf nova username nova
openstack-config --set /etc/neutron/neutron.conf nova password nova
openstack-config --set /etc/neutron/neutron.conf oslo_concurrency lock_path /var/lib/neutron/tmp

# 修改plugins/ml2/ml2_conf.ini
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 type_drivers flat,vlan,vxlan
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 tenant_network_types vxlan,flat
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 mechanism_drivers openvswitch,l2population
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 extension_drivers port_security
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_flat flat_networks '*'
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_vxlan vni_ranges 2001:4000
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_ipset True

# 修改plugins/ml2/openvswitch_agent.ini
export my_tenant_ip=`ifconfig eth2 | grep inet -w | awk '{print $2}'`
openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini agent tunnel_types vxlan
openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini agent l2_population True
openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini agent arp_responder True
openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini agent enable_distributed_routing True
openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini agent tunnel_csum True
openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini ovs of_interface native
openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini ovs ovsdb_interface native
openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini ovs local_ip $my_tenant_ip
openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini ovs bridge_mappings physnet1:br-ex
openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini securitygroup \
    firewall_driver neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver

#修改l3_agent.ini
openstack-config --set /etc/neutron/l3_agent.ini DEFAULT interface_driver openvswitch
openstack-config --set /etc/neutron/l3_agent.ini DEFAULT ha_vrrp_auth_password password

# 修改metadata_agent.ini
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT nova_metadata_ip controller
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT metadata_proxy_shared_secret neutron
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT metadata_workers 2

# 添加dhcp的配置文件。由于我们使用的是vxlan网络，需要强制设置虚拟机的mtu为1450
cat >/etc/neutron/dnsmasq.conf<<EOF
dhcp-option-force=26,1450
EOF

# 修改dhcp_agent.ini
openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT interface_driver openvswitch
openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT dhcp_driver neutron.agent.linux.dhcp.Dnsmasq
openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT enable_isolated_metadata True
openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT dnsmasq_config_file /etc/neutron/dnsmasq.conf

# 创建文件链接
ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini
\end{code-in-enumerate}

\1 网络节点-即controller的安装配置
\begin{code-in-enumerate}{bash}
# 设置数据库
mysql
CREATE DATABASE neutron;
GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' IDENTIFIED BY 'neutron';
GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' IDENTIFIED BY 'neutron';
GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'controller' IDENTIFIED BY 'neutron';

# 添加neutron 服务
openstack service create --name neutron --description "OpenStack Networking" network
openstack endpoint create --region wuhan network public http://controller:9696
openstack endpoint create --region wuhan network internal http://controller:9696
openstack endpoint create --region wuhan network admin http://controller:9696

# 添加neutron用户
openstack user create --domain default --project service --project-domain default \
    --password neutron neutron
openstack role add --project service --user neutron --project-domain default \
    --user-domain default admin

# 修改l3_agent.ini
openstack-config --set /etc/neutron/l3_agent.ini DEFAULT agent_mode dvr_snat

# 同步数据库
neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file \
    /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head

# 修改文件所有者
chown -R neutron:neutron /etc/neutron /var/lib/neutron /var/log/neutron
#启动服务
for id in neutron-{server,openvswitch-agent,dhcp-agent,metadata-agent,l3-agent};\
    do systemctl enable $id;systemctl start $id;done
\end{code-in-enumerate}

\1 计算节点的配置
\begin{code-in-enumerate}{bash}
# 修改l3_agent.ini
openstack-config --set /etc/neutron/l3_agent.ini DEFAULT agent_mode dvr

# 修改文件所有者
chown -R neutron:neutron /etc/neutron /var/lib/neutron /var/log/neutron

# 启动服务
for id in neutron-{openvswitch,metadata,l3}-agent;do systemctl enable $id;\
    systemctl start $id;done
\end{code-in-enumerate}

\end{outline}

\section{Nova安装配置}
\begin{outline}[enumerate]

\1 通用安装-所有节点
\begin{code-in-enumerate}{bash}
yum install openstack-nova python-nova openstack-utils -y
export my_ip=`ifconfig br-ex | grep inet -w | awk '{print $2}'`
export my_block_storage_ip=`ifconfig eth1 | grep inet -w | awk '{print $2}'`
openstack-config --set /etc/nova/nova.conf DEFAULT my_ip $my_ip
openstack-config --set /etc/nova/nova.conf DEFAULT my_block_storage_ip $my_block_storage_ip
openstack-config --set /etc/nova/nova.conf DEFAULT use_neutron True
openstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver
openstack-config --set /etc/nova/nova.conf DEFAULT linuxnet_interface_driver \
    nova.network.linux_net.LinuxOVSInterfaceDriver

openstack-config --set /etc/nova/nova.conf DEFAULT scheduler_default_filters \
    RetryFilter,AvailabilityZoneFilter,ComputeFilter,ImagePropertiesFilter,\
    ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter,AggregateMultiTenancyIsolation,\
    AggregateInstanceExtraSpecsFilter,AggregateCoreFilter,AggregateRamFilter
openstack-config --set /etc/nova/nova.conf DEFAULT reclaim_instance_interval 7200
openstack-config --set /etc/nova/nova.conf DEFAULT resize_confirm_window 1
openstack-config --set /etc/nova/nova.conf DEFAULT flat_injected True
openstack-config --set /etc/nova/nova.conf DEFAULT injected_network_template \
    '$pybasedir/nova/virt/interfaces.template'
openstack-config --set /etc/nova/nova.conf DEFAULT force_config_drive true

openstack-config --set /etc/nova/nova.conf DEFAULT enabled_apis osapi_compute,metadata
openstack-config --set /etc/nova/nova.conf DEFAULT transport_url rabbit://guest:guest@controller
openstack-config --set /etc/nova/nova.conf DEFAULT osapi_compute_workers 2
openstack-config --set /etc/nova/nova.conf DEFAULT metadata_workers 2

openstack-config --set /etc/nova/nova.conf database connection \
    mysql+pymysql://nova:nova@controller/nova
openstack-config --set /etc/nova/nova.conf api_database connection \
    mysql+pymysql://nova:nova@controller/nova_api
openstack-config --set /etc/nova/nova.conf api auth_strategy keystone
openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_uri http://controller:5000
openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_url http://controller:35357
openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_type password
openstack-config --set /etc/nova/nova.conf keystone_authtoken project_domain_name default
openstack-config --set /etc/nova/nova.conf keystone_authtoken user_domain_name default
openstack-config --set /etc/nova/nova.conf keystone_authtoken project_name service
openstack-config --set /etc/nova/nova.conf keystone_authtoken username nova
openstack-config --set /etc/nova/nova.conf keystone_authtoken password nova

openstack-config --set /etc/nova/nova.conf vnc enabled true
openstack-config --set /etc/nova/nova.conf vnc vncserver_listen 0.0.0.0
openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address $my_ip

openstack-config --set /etc/nova/nova.conf glance api_servers http://controller:9292
openstack-config --set /etc/nova/nova.conf oslo_concurrency lock_path /var/lib/nova/tmp

openstack-config --set /etc/nova/nova.conf neutron url http://controller:9696
openstack-config --set /etc/nova/nova.conf neutron auth_url http://controller:35357
openstack-config --set /etc/nova/nova.conf neutron auth_type password
openstack-config --set /etc/nova/nova.conf neutron project_domain_name default
openstack-config --set /etc/nova/nova.conf neutron user_domain_name default
openstack-config --set /etc/nova/nova.conf neutron region_name wuhan
openstack-config --set /etc/nova/nova.conf neutron project_name neutron
openstack-config --set /etc/nova/nova.conf neutron username neutron
openstack-config --set /etc/nova/nova.conf neutron project_name service
openstack-config --set /etc/nova/nova.conf neutron password neutron
openstack-config --set /etc/nova/nova.conf neutron service_metadata_proxy true
openstack-config --set /etc/nova/nova.conf neutron metadata_proxy_shared_secret neutron

openstack-config --set /etc/nova/nova.conf cinder os_region_name wuhan
openstack-config --set /etc/nova/nova.conf cinder catalog_info volumev2:cinderv2:internalURL
openstack-config --set /etc/nova/nova.conf libvirt images_rbd_pool volumes
openstack-config --set /etc/nova/nova.conf libvirt images_rbd_ceph_conf /etc/ceph/ceph.conf
openstack-config --set /etc/nova/nova.conf libvirt rbd_user awcloud
openstack-config --set /etc/nova/nova.conf libvirt rbd_secret_uuid a7f64266-0894-4f1e-a635-d0aeaca0e993

openstack-config --set /etc/nova/nova.conf conductor workers 2

openstack-config --set /etc/nova/nova.conf cache enabled true
openstack-config --set /etc/nova/nova.conf cache memcache_servers controller:11211
openstack-config --set /etc/nova/nova.conf cache backend oslo_cache.memcache_pool
openstack-config --set /etc/nova/nova.conf cache debug_cache_backend true
openstack-config --set /etc/nova/nova.conf cache expiration_time 600

# 如果是在虚拟机环境当中，则还需添加如下的设置
openstack-config --set /etc/nova/nova.conf libvirt virt_type qemu
openstack-config --set /etc/nova/nova.conf libvirt cpu_mode none
\end{code-in-enumerate}

\1 controller节点安装配置
\begin{code-in-enumerate}{bash}
export my_ip=`ifconfig br-ex | grep inet -w | awk '{print $2}'`
openstack-config --set /etc/nova/nova.conf vnc novncproxy_base_url http://$my_ip:6080/vnc_auto.html

# 创建数据库
mysql
CREATE DATABASE nova_api;
CREATE DATABASE nova;
GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' IDENTIFIED BY 'nova';
GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' IDENTIFIED BY 'nova';
GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'controller' IDENTIFIED BY 'nova';
GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' IDENTIFIED BY 'nova';
GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' IDENTIFIED BY 'nova';
GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'controller' IDENTIFIED BY 'nova';

# 创建nova service
openstack service create --name nova --description "OpenStack Compute" compute
openstack service create --name placement --description "Placement API" placement

# 创建nova的endpoint
openstack endpoint create --region wuhan compute public http://controller:8774/v2.1
openstack endpoint create --region wuhan compute internal http://controller:8774/v2.1
openstack endpoint create --region wuhan compute admin http://controller:8774/v2.1

# 添加nova用户
openstack user create --domain default --project service --project-domain default \
    --password nova nova
openstack role add --project service --user nova --project-domain default \
    --user-domain default admin

# nova数据库初始化
nova-manage api_db sync
nova-manage db sync

# 修改文件所有者
chown -R nova:nova /etc/nova /var/lib/nova /var/log/nova

# 设置开机自启动并启动服务
for id in openstack-nova-{conductor,api,scheduler,consoleauth,novncproxy};\
    do systemctl enable $id;systemctl start $id;done
\end{code-in-enumerate}

\1 计算节点安装配置
\begin{code-in-enumerate}{bash}
openstack-config --set /etc/nova/nova.conf vnc novncproxy_base_url \
    http://<controller的br-ex上的ip地址>:6080/vnc_auto.html
systemctl enable libvirtd
systemctl start libvirtd
cd /opt
export secret_id=a7f64266-0894-4f1e-a635-d0aeaca0e993
cat > secret.xml <<EOF
    <secret ephemeral='no' private='no'>
      <uuid>$secret_id</uuid>
      <usage type='ceph'>
        <name>client.awcloud</name>
      </usage>
    </secret>
EOF
virsh secret-define --file secret.xml
ceph auth get-key client.awcloud | tee client.awcloud.key
virsh secret-set-value --secret $secret_id --base64 $(cat client.awcloud.key)

chown -R nova:nova /etc/nova /var/lib/nova /var/log/nova
for id in openstack-nova-{conductor,scheduler,compute};do systemctl enable $id;\
    systemctl start $id;done
\end{code-in-enumerate}

\end{outline}
