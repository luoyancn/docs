\chapter{Kubernets}
Kubernetes是Google开源的容器集群管理软件，可以方便的管理容器以及容器集群。

\section{部署的架构}
Kubernetes可以使用单节点部署，但这种模式一般只用于测试环境。而在实际的生产环境中，
必须使用多节点的方式安装。Kubernetes的最小集群需要4台节点：1个monitor，3个nodes。
每个节点的具体作用如下表。
\begin{center}
  \rowcolors{2}{green!80!yellow!50}{green!70!yellow!40}
  \begin{tabularx}{\textwidth}{|l|l|X|}
  \hline
  IP & HostName & Services\\ \hline
  172.16.1.158 & k8smon & kube-apiserver, kube-scheduler, kube-controller-manager, kube-dns, etcd, flanneld\\
  172.16.1.155 & k8s1 & kube-proxy, kubelet, flanneld, docker \\
  172.16.1.156 & k8s2 & kube-proxy, kubelet, flanneld, docker \\
  172.16.1.157 & k8s3 & kube-proxy, kubelet, flanneld, docker \\
  \hline
  \end{tabularx}
  \label{tab:URL Mapping}
\end{center}

\section{通用安装}

\begin{outline}[enumerate]
  \1 配置yum源（\textattachfile{cern.repo}{\textcolor{blue}{cern.repo}}和\textattachfile{docker.repo}{\textcolor{blue}{docker.repo}}，如附件）和安装通用软件
\begin{code-in-enumerate}{bash}
cp cern.repo /etc/yum.repos.d
cp docker.repo /etc/yum.repos.d
rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
yum install http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm \
    https://repos.fedorapeople.org/repos/openstack/openstack-ocata/rdo-release-ocata-2.noarch.rpm -y
yum erase dnsmasq -y
yum update -y;yum install crudini -y
crudini --set elrepo.repo elrepo enabled 1
crudini --set elrepo.repo elrepo-kernel enabled 1
crudini --set elrepo.repo elrepo-extras enabled 1
yum install kernel-ml etcd flannel docker-engine-1.12.6-1.el7.centos.x86_64 \
    docker-engine-selinux-1.12.6-1.el7.centos -y
# 防止update的时候，用community版本的docker-engine替换相关的软件包
mv docker.repo docker.repo_bak
reboot
\end{code-in-enumerate}

  \1 修改系统参数
\begin{code-in-enumerate}{bash}
# 修改主机名
echo $HOSTNAME > /etc/hostname
# 关闭并禁用防火墙
systemctl disable firewalld;systemctl stop firewalld
# 禁用selinux
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config

# 修改内核参数
cat >> /etc/sysctl.conf <<EOF
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv4.ip_forward = 1
EOF

# 加载内核模块
echo "overlay" > /etc/modules-load.d/overlay.conf
reboot
\end{code-in-enumerate}

  \1 配置docker
\begin{code-in-enumerate}{bash}
cat > /etc/sysconfig/docker-network <<EOF
# /etc/sysconfig/docker-network
DOCKER_NETWORK_OPTIONS=
EOF

crudini --set /usr/lib/systemd/system/docker.service Service EnvironmentFile -/etc/sysconfig/docker-network
# 添加下面的docker配置
ExecStart=/usr/bin/dockerd \
          $DOCKER_NETWORK_OPTIONS \
          -H unix:///var/run/docker.sock \
          -H tcp://0.0.0.0:2375 \
          --storage-driver=overlay \
          --selinux-enabled=false \
          --insecure-registry=10.1.1.16:5000
systemctl daemon-reload
systemctl enable docker
# 不能在这个地方启动docker，因为后续的flanneld有影响。
\end{code-in-enumerate}

  \1 配置etcd
\begin{code-in-enumerate}{bash}
# K8SMON 表示kubernetes mon的地址
sed -i \
    's/ETCD_LISTEN_CLIENT_URLS="http:\/\/localhost:2379"/ETCD_LISTEN_CLIENT_URLS="http:\/\/0.0.0.0:2379"/g' \
    /etc/etcd/etcd.conf
sed -i \
    's/ETCD_ADVERTISE_CLIENT_URLS="http:\/\/localhost:2379"/ETCD_ADVERTISE_CLIENT_URLS="http:\/\/10.1.1.16:2379"/g' \
    /etc/etcd/etcd.conf
systemctl enable etcd;systemctl start etcd
# 验证安装
etcdctl --endpoints http://10.1.1.16:2379 member list
\end{code-in-enumerate}

  \1 设置flanneld使用的网段

Docker默认会在本机新建一个docker0网桥，默认网段为172.17.0.1/16，可以通过dockerd的 --bip参数指定。
想要docker容器跨节点通信，需要对docker的网络重新划分。Flanneld实现了一个扁平的网络（10.1.0.0/16），
重新配置docker的网桥，使每个节点的docker网桥的网段都是属于这个大网络的子网。 这样每个容器的ip都属于
同一个网络内（10.1.0.0/16），可以直接使用ip通信，而跨节点的功能是flanneld实现并对docker透明。
\begin{code-in-enumerate}{bash}
etcdctl set /k8s/network/config '{"NetWork":"108.8.0.0/16"}'
\end{code-in-enumerate}
命令的含义是期望docker运行的container实例的地址，都在 10.1.0.0/16 网段中。
Flanneld会读取/kubs/network目录中config的值，然后接管docker的地址分配，并把docker和宿主机器之间的网络桥接起来。
也可以按照Google的方式添加网络：
\begin{code-in-enumerate}{bash}
etcdctl mkdir /kubs/network
etcdctl mk /kubs/network/config \
    "{ \"Network\": \"10.1.0.0/16\", \"SubnetLen\": 24, \"Backend\": { \"Type\": \"vxlan\" } }"
\end{code-in-enumerate}

  \1 配置flanneld
\begin{code-in-enumerate}{bash}
# K8SMON 表示kubernetes mon的地址
sed -i 's/127.0.0.1/10.1.1.16/g' /etc/sysconfig/flanneld
sed -i 's/atomic.io/k8s/g' /etc/sysconfig/flanneld
systemctl enable flanneld;systemctl start flanneld;systemctl start docker
\end{code-in-enumerate}

  \1 验证flannel安装
\begin{code-in-enumerate}{bash}
[root@k8smon log]# etcdctl ls /k8s/network/subnets
/kubs/network/subnets/10.1.30.0-24
/kubs/network/subnets/10.1.10.0-24
/kubs/network/subnets/10.1.28.0-24
/kubs/network/subnets/10.1.76.0-24
[root@k8smon log]# etcdctl get /k8s/network/subnets/10.1.30.0-24
{"PublicIP":"172.16.1.155"}
[root@k8smon log]# etcdctl get /k8s/network/subnets/10.1.10.0-24
{"PublicIP":"172.16.1.157"}
[root@k8smon log]# etcdctl get /k8s/network/subnets/10.1.28.0-24
{"PublicIP":"172.16.1.156"}
[root@k8smon log]# etcdctl get /k8s/network/subnets/10.1.76.0-24
{"PublicIP":"172.16.1.158"}
[root@k8smon ~]# ifconfig docker0
docker0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
        inet 10.1.76.1  netmask 255.255.255.0  broadcast 0.0.0.0
        ether 02:42:81:22:bf:2d  txqueuelen 0  (Ethernet)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
\end{code-in-enumerate}

  \1 安装kubernetes软件
\begin{code-in-enumerate}{bash}
tar -zxvf kubernetes-server-linux-amd64.tar.gz
cp /opt/kubernetes/server/bin/hyperkube /usr/bin
cp /opt/kubernetes/server/bin/kubeadm /usr/bin
cp /opt/kubernetes/server/bin/kube-apiserver /usr/bin
cp /opt/kubernetes/server/bin/kube-controller-manager /usr/bin
cp /opt/kubernetes/server/bin/kubectl /usr/bin
cp /opt/kubernetes/server/bin/kube-discovery /usr/bin
cp /opt/kubernetes/server/bin/kube-dns /usr/bin
cp /opt/kubernetes/server/bin/kubefed /usr/bin
cp /opt/kubernetes/server/bin/kubelet /usr/bin
cp /opt/kubernetes/server/bin/kube-proxy /usr/bin
cp /opt/kubernetes/server/bin/kube-scheduler /usr/bin
chmod +x /usr/bin/kube*
chmod +x /usr/bin/hyperkube
\end{code-in-enumerate}

  \1 添加kubernetes用户及相关路径
\begin{code-in-enumerate}{bash}
groupadd -r kube
useradd -r -g kube -d / -s /sbin/nologin -c "Kubernetes user" kube
mkdir -p /etc/kubernetes /var/run/kubernetes /var/lib/kube-dns /var/lib/kubelet
cat >/etc/kubernetes/config<<EOF
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR="--logtostderr=true"
# journal message level, 0 is debug
KUBE_LOG_LEVEL="--v=0"
# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV="--allow-privileged=true"
# How the controller-manager, scheduler, and proxy find the apiserver
KUBE_MASTER="--master=http://10.1.1.16:8080"
EOF
chown -R kube:kube /etc/kubernetes /var/run/kubernetes /var/lib/kube-dns /var/lib/kubelet
\end{code-in-enumerate}

\end{outline}

\section{Kubernetes安装}
根据服务器的角色不同，kubernets分为monitor和node。这2种服务器的安装方式有一些区别。

\subsection{Monitor的配置}
\begin{code-block}{bash}
cat >/etc/kubernetes/apiserver<<EOF
###
# kubernetes system config
#
# The following values are used to configure the kube-apiserver
#
# The address on the local server to listen to.
KUBE_API_ADDRESS="--insecure-bind-address=0.0.0.0"
# The port on the local server to listen on.
KUBE_API_PORT="--port=8080"
# Port minions listen on
KUBELET_PORT="--kubelet-port=10250"
# Comma separated list of nodes in the etcd cluster
KUBE_ETCD_SERVERS="--etcd-servers=http://10.1.1.16:2379"
# Address range to use for services
# service cluster ip一定不能和etcd的ip range冲突！！
KUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=108.36.0.0/16"
# default admission control policies
KUBE_ADMISSION_CONTROL="--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota"
KUBE_API_ARGS="--client-ca-file=/etc/kubernetes/credentials/ca.crt \\
               --tls-private-key-file=/etc/kubernetes/credentials/server.key \\
               --tls-cert-file=/etc/kubernetes/credentials/server.crt"
EOF

cat >/etc/kubernetes/controller-manager<<EOF
KUBE_CONTROLLER_MANAGER_ARGS="--root-ca-file=/etc/kubernetes/credentials/ca.crt \\
    --service-account-private-key-file=/etc/kubernetes/credentials/server.key"
EOF

cat >/etc/kubernetes/scheduler<<EOF
KUBE_SCHEDULER_ARGS=""
EOF

cat >/etc/kubernetes/dns<<EOF
KUBE_DNS_PORT="--dns-port=53"
KUBE_DNS_DOMAIN="--domain=k8s.centos.me"
KUBE_DNS_MASTER="--kube-master-url=http://10.1.1.16:8080"
KUBE_DNS_ARGS=""
EOF

mkdir -p /etc/kubernetes/credentials
cd /etc/kubernetes/credentials
export MASTER_IP="10.1.1.16"
export MASTER_NAME="k8smon"
openssl genrsa -out ca.key 2048
openssl req -x509 -new -nodes -key ca.key -subj "/CN=${MASTER_IP}" -days 10000 -out ca.crt
openssl genrsa -out server.key 2048
openssl req -new -key server.key -subj "/CN=${MASTER_NAME}" -out server.csr
openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 10000

cat >/usr/lib/systemd/system/kube-apiserver.service<<EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
After=etcd.service
[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/apiserver
User=kube
ExecStart=/usr/bin/kube-apiserver \\
            \$KUBE_LOGTOSTDERR \\
            \$KUBE_LOG_LEVEL \\
            \$KUBE_ETCD_SERVERS \\
            \$KUBE_API_ADDRESS \\
            \$KUBE_API_PORT \\
            \$KUBELET_PORT \\
            \$KUBE_ALLOW_PRIV \\
            \$KUBE_SERVICE_ADDRESSES \\
            \$KUBE_ADMISSION_CONTROL \\
            \$KUBE_API_ARGS
Restart=on-failure
Type=notify
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
EOF

cat >/usr/lib/systemd/system/kube-controller-manager.service<<EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/controller-manager
User=kube
ExecStart=/usr/bin/kube-controller-manager \\
            \$KUBE_LOGTOSTDERR \\
            \$KUBE_LOG_LEVEL \\
            \$KUBE_MASTER \\
            \$KUBE_CONTROLLER_MANAGER_ARGS
Restart=on-failure
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
EOF

cat >/usr/lib/systemd/system/kube-scheduler.service<<EOF
[Unit]
Description=Kubernetes Scheduler Plugin
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/scheduler
User=kube
ExecStart=/usr/bin/kube-scheduler \\
            \$KUBE_LOGTOSTDERR \\
            \$KUBE_LOG_LEVEL \\
            \$KUBE_MASTER \\
            \$KUBE_SCHEDULER_ARGS
Restart=on-failure
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
EOF

cat >/usr/lib/systemd/system/kube-dns.service<<EOF
[Unit]
Description=Kubernetes Kube-dns Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=kube-apiserver.service
Requires=kube-apiserver.service
[Service]
WorkingDirectory=/var/lib/kube-dns
EnvironmentFile=-/etc/kubernetes/dns
ExecStart=/usr/bin/kube-dns \\
            \$KUBE_DNS_PORT \\
            \$KUBE_DNS_DOMAIN \\
            \$KUBE_DNS_MASTER \\
            \$KUBE_DNS_ARGS
Restart=on-failure
[Install]
WantedBy=multi-user.target
EOF

chown -R kube:kube /etc/kubernetes /var/run/kubernetes /var/lib/kube-dns /var/lib/kubelet

systemctl daemon-reload
for id in kube-{apiserver,scheduler,controller-manager,dns};\
    do systemctl enable $id;systemctl start $id;done
\end{code-block}

\subsection{Node的配置}
\begin{code-block}{bash}
cat >/etc/kubernetes/kubelet<<EOF
#### kubernetes kubelet (minion) config
# The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)
KUBELET_ADDRESS="--address=0.0.0.0"
# The port for the info server to serve on
KUBELET_PORT="--port=10250"
# You may leave this blank to use the actual hostname
KUBELET_HOSTNAME="--hostname-override="
# location of the api-server
KUBELET_API_SERVER="--api-servers=http://10.1.1.16:8080"
# pod infrastructure container
KUBELET_POD_INFRA_CONTAINER="\
    --pod-infra-container-image=10.1.1.16:5000/rhel7/pod-infrastructure:latest"
# Add your own!
KUBELET_ARGS="--cluster-dns=10.1.1.16 --cluster-domain=k8s.centos.me"
EOF

cat >/etc/kubernetes/proxy<<EOF
#### kubernetes proxy config
# default config should be adequate
# Add your own!
KUBE_PROXY_ARGS=""
EOF

cat >/usr/lib/systemd/system/kubelet.service<<EOF
[Unit]
Description=Kubernetes Kubelet Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service
[Service]
WorkingDirectory=/var/lib/kubelet
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/kubelet
ExecStart=/usr/bin/kubelet \\
            \$KUBE_LOGTOSTDERR \\
            \$KUBE_LOG_LEVEL \\
            \$KUBELET_API_SERVER \\
            \$KUBELET_ADDRESS \\
            \$KUBELET_PORT \\
            \$KUBELET_HOSTNAME \\
            \$KUBE_ALLOW_PRIV \\
            \$KUBELET_POD_INFRA_CONTAINER \\
            \$KUBELET_ARGS
Restart=on-failure
[Install]
WantedBy=multi-user.target
EOF

cat >/usr/lib/systemd/system/kube-proxy.service<<EOF
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/proxy
ExecStart=/usr/bin/kube-proxy \\
            \$KUBE_LOGTOSTDERR \\
            \$KUBE_LOG_LEVEL \\
            \$KUBE_MASTER \\
            \$KUBE_PROXY_ARGS
Restart=on-failure
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
EOF

chown -R kube:kube /etc/kubernetes /var/run/kubernetes /var/lib/kube-dns /var/lib/kubelet

systemctl daemon-reload
for id in {kubelet,kube-proxy};\
    do systemctl enable $id;systemctl start $id;done
\end{code-block}

\section{Kubernetes的基本使用}
\begin{code-block}{bash}
# 查看k8s集群的节点
[root@k8smon ~]# kubectl get nodes
NAME      STATUS    AGE
k8s1      Ready     1d
k8s2      Ready     1d
k8s3      Ready     1d

# 查看命名空间
[root@k8smon ~]# kubectl get namespace
NAME          STATUS    AGE
default       Active    1d
kube-system   Active    1d

# 如果没有kube-system，则需要新建
cat >kubu-system-ns.yaml<<EOF
apiVersion: v1
kind: Namespace
metadata:
  name: kube-system
EOF
kubectl create -f kube-system-ns.yaml
\end{code-block}

k8s的基础有了之后，可以进行进一步的操作，所使用的yaml文件如附件：\textattachfile{k8s-dashboard-rc.yaml}{\textcolor{blue}{k8s-dashboard-rc.yaml}}和
\textattachfile{k8s-dashboard-svc.yaml}{\textcolor{blue}{k8s-dashboard-svc.yaml}}
\begin{code-block}{bash}
# 创建一个rc
kubectl create -f k8s-dashboard-rc.yaml
[root@k8smon ~]# kubectl get rc --namespace kube-system
NAME                          DESIRED   CURRENT   READY     AGE
kubernetes-dashboard-v1.4.0   3         3         3         32m

[root@k8smon ~]# kubectl get pods --namespace kube-system
NAME                                READY     STATUS    RESTARTS   AGE
kubernetes-dashboard-v1.4.0-2h729   1/1       Running   0          34m
kubernetes-dashboard-v1.4.0-30qlt   1/1       Running   1          34m
kubernetes-dashboard-v1.4.0-v6xl8   1/1       Running   0          34m

# 创建一个service
kubectl create -f k8s-dashboard-svc.yaml
[root@k8smon ~]# kubectl get svc --namespace kube-system
NAME                   CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes-dashboard   100.1.133.196   <nodes>       80:30000/TCP   26m
\end{code-block}

\section{Commands of Kubernetes}
\begin{code-block}{bash}
# 查询deployments
kubectl get --namespace kube-system deployments

# 查询单个的deployment
kubectl get --namespace kube-system deployments nginx-ingress-controller

# 显示单个的deployment
kubectl describe --namespace kube-system deployments nginx-ingress-controller

# 进入pods的container
kubectl exec -it pods --container container1 -- /bin/bash

\end{code-block}
