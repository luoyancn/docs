\chapter{Kubernets}
Kubernetes是Google开源的容器集群管理软件，可以方便的管理容器以及容器集群。

\section{部署的架构}
Kubernetes可以使用单节点部署，但这种模式一般只用于测试环境。而在实际的生产环境中，
必须使用多节点的方式安装。Kubernetes的最小集群需要4台节点：1个monitor，3个nodes。
每个节点的具体作用如下表。
\begin{center}
  \rowcolors{2}{green!80!yellow!50}{green!70!yellow!40}
  \begin{tabularx}{\textwidth}{|l|l|X|}
  \hline
  IP & HostName & Services\\ \hline
  172.16.1.158 & k8smon & kube-apiserver, kube-scheduler, kube-controller-manager, kube-dns, etcd, flanneld\\
  172.16.1.155 & k8s1 & kube-proxy, kubelet, flanneld, docker \\
  172.16.1.156 & k8s2 & kube-proxy, kubelet, flanneld, docker \\
  172.16.1.157 & k8s3 & kube-proxy, kubelet, flanneld, docker \\
  \hline
  \end{tabularx}
  \label{tab:URL Mapping}
\end{center}

\section{通用安装}

\begin{outline}[enumerate]
  \1 配置yum源（\textattachfile{cern.repo}{\textcolor{blue}{cern.repo}}和\textattachfile{docker.repo}{\textcolor{blue}{docker.repo}}，如附件）和安装通用软件
\begin{code-in-enumerate}{bash}
cp cern.repo /etc/yum.repos.d
cp docker.repo /etc/yum.repos.d
rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
yum install http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm \
    https://repos.fedorapeople.org/repos/openstack/openstack-ocata/rdo-release-ocata-2.noarch.rpm -y
yum erase dnsmasq -y
yum update -y;yum install crudini -y
crudini --set elrepo.repo elrepo enabled 1
crudini --set elrepo.repo elrepo-kernel enabled 1
crudini --set elrepo.repo elrepo-extras enabled 1
yum install kernel-ml etcd flannel docker-engine-1.12.6-1.el7.centos.x86_64 \
    docker-engine-selinux-1.12.6-1.el7.centos -y
# 防止update的时候，用community版本的docker-engine替换相关的软件包
mv docker.repo docker.repo_bak
reboot
\end{code-in-enumerate}

  \1 修改系统参数
\begin{code-in-enumerate}{bash}
# 修改主机名
echo $HOSTNAME > /etc/hostname
# 关闭并禁用防火墙
systemctl disable firewalld;systemctl stop firewalld
# 禁用selinux
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config

# 修改内核参数
cat >> /etc/sysctl.conf <<EOF
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv4.ip_forward = 1
EOF

# 加载内核模块
echo "overlay" > /etc/modules-load.d/overlay.conf
reboot
\end{code-in-enumerate}

  \1 配置docker
\begin{code-in-enumerate}{bash}
cat > /etc/sysconfig/docker-network <<EOF
# /etc/sysconfig/docker-network
DOCKER_NETWORK_OPTIONS=
EOF

crudini --set /usr/lib/systemd/system/docker.service Service EnvironmentFile -/etc/sysconfig/docker-network
# 添加下面的docker配置
ExecStart=/usr/bin/dockerd \
          $DOCKER_NETWORK_OPTIONS \
          -H unix:///var/run/docker.sock \
          -H tcp://0.0.0.0:2375 \
          --storage-driver=overlay \
          --selinux-enabled=false \
          --insecure-registry=10.1.1.16:5000
systemctl daemon-reload
systemctl enable docker
# 不能在这个地方启动docker，因为后续的flanneld有影响。
\end{code-in-enumerate}

  \1 配置etcd
\begin{code-in-enumerate}{bash}
cat >/opt/etcd_cluster.sh<<EOF
#!/usr/bin/env bash
set -e
# set the IP of each member of etcd cluster.
etcd1=10.1.1.16
etcd2=10.1.1.17
etcd3=10.1.1.18

etcd_initial_cluster="etcd1=http://${etcd1}:2380,etcd2=http://${etcd2}:2380,etcd3=http://${etcd3}:2380"
etcd_cluster_name="etcd-cluster-k8s"
for etcd_name in etcd1 etcd2 etcd3; do
    curr_host_ip=${!etcd_name}
    ssh ${curr_host_ip} -o StrictHostKeyChecking=no "
    sed -i -r \
        -e 's|.*(ETCD_NAME=).*|\1\"'${etcd_name}'\"|' \
        -e 's|.*(ETCD_DATA_DIR=).*|\1\"/var/lib/etcd/'${etcd_name}'\"|' \
        -e 's|.*(ETCD_LISTEN_PEER_URLS=).*|\1\"http://'${curr_host_ip}':2380\"|' \
        -e 's|.*(ETCD_LISTEN_CLIENT_URLS=).*|\1\"http://'${curr_host_ip}':2379,http://127.0.0.1:2379\"|' \
        -e 's|.*(ETCD_INITIAL_ADVERTISE_PEER_URLS=).*|\1\"http://'${curr_host_ip}':2380\"|' \
        -e 's|.*(ETCD_INITIAL_CLUSTER=).*|\1\"'${etcd_initial_cluster}'\"|' \
        -e 's|.*(ETCD_INITIAL_CLUSTER_STATE=).*|\1\"new\"|' \
        -e 's|.*(ETCD_INITIAL_CLUSTER_TOKEN=).*|\1\"'${etcd_cluster_name}'\"|' \
        -e 's|.*(ETCD_ADVERTISE_CLIENT_URLS=).*|\1\"http://'${curr_host_ip}':2379\"|' \
        /etc/etcd/etcd.conf
"
done
EOF
systemctl enable etcd;systemctl start etcd
# 验证安装
etcdctl member list
etcdctl cluster-health
\end{code-in-enumerate}
注意，该配置默认使用集群模式，需要相关节点做ssh互信。ssh互信的配置不再赘述。
配置etcd的集群也可以使用附件当中的脚本\textattachfile{etcd-cluster.sh}{\textcolor{blue}{etcd-cluster.sh}}进行自动化的配置。


  \1 设置flanneld使用的网段

Docker默认会在本机新建一个docker0网桥，默认网段为172.17.0.1/16，可以通过dockerd的 --bip参数指定。
想要docker容器跨节点通信，需要对docker的网络重新划分。Flanneld实现了一个扁平的网络（10.1.0.0/16），
重新配置docker的网桥，使每个节点的docker网桥的网段都是属于这个大网络的子网。 这样每个容器的ip都属于
同一个网络内（10.1.0.0/16），可以直接使用ip通信，而跨节点的功能是flanneld实现并对docker透明。
\begin{code-in-enumerate}{bash}
etcdctl set /k8s/network/config '{"NetWork":"108.8.0.0/16"}'
\end{code-in-enumerate}
命令的含义是期望docker运行的container实例的地址，都在 10.1.0.0/16 网段中。
Flanneld会读取/kubs/network目录中config的值，然后接管docker的地址分配，并把docker和宿主机器之间的网络桥接起来。
也可以按照Google的方式添加网络：
\begin{code-in-enumerate}{bash}
etcdctl mkdir /kubs/network
etcdctl mk /kubs/network/config \
    "{ \"Network\": \"10.1.0.0/16\", \"SubnetLen\": 24, \"Backend\": { \"Type\": \"vxlan\" } }"
\end{code-in-enumerate}

  \1 配置flanneld
\begin{code-in-enumerate}{bash}
sed -i 's/atomic.io/k8s/g' /etc/sysconfig/flanneld
sed -i \
    's/http:\/\/127.0.0.1:2379/http:\/\/10.1.1.16:2379,http:\/\/10.1.1.17:2379,http:\/\/10.1.1.18:2379/g' \
    /etc/sysconfig/flanneld
systemctl enable flanneld;systemctl start flanneld;systemctl start docker
\end{code-in-enumerate}

  \1 验证flannel安装
\begin{code-in-enumerate}{bash}
[root@k8smon log]# etcdctl ls /k8s/network/subnets
/kubs/network/subnets/10.1.30.0-24
/kubs/network/subnets/10.1.10.0-24
/kubs/network/subnets/10.1.28.0-24
/kubs/network/subnets/10.1.76.0-24
[root@k8smon log]# etcdctl get /k8s/network/subnets/10.1.30.0-24
{"PublicIP":"172.16.1.155"}
[root@k8smon log]# etcdctl get /k8s/network/subnets/10.1.10.0-24
{"PublicIP":"172.16.1.157"}
[root@k8smon log]# etcdctl get /k8s/network/subnets/10.1.28.0-24
{"PublicIP":"172.16.1.156"}
[root@k8smon log]# etcdctl get /k8s/network/subnets/10.1.76.0-24
{"PublicIP":"172.16.1.158"}
[root@k8smon ~]# ifconfig docker0
docker0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
        inet 10.1.76.1  netmask 255.255.255.0  broadcast 0.0.0.0
        ether 02:42:81:22:bf:2d  txqueuelen 0  (Ethernet)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
\end{code-in-enumerate}

  \1 安装kubernetes软件
\begin{code-in-enumerate}{bash}
tar -zxvf kubernetes-server-linux-amd64.tar.gz
cp /opt/kubernetes/server/bin/hyperkube /usr/bin
cp /opt/kubernetes/server/bin/kubeadm /usr/bin
cp /opt/kubernetes/server/bin/kube-apiserver /usr/bin
cp /opt/kubernetes/server/bin/kube-controller-manager /usr/bin
cp /opt/kubernetes/server/bin/kubectl /usr/bin
cp /opt/kubernetes/server/bin/kube-discovery /usr/bin
cp /opt/kubernetes/server/bin/kube-dns /usr/bin
cp /opt/kubernetes/server/bin/kubefed /usr/bin
cp /opt/kubernetes/server/bin/kubelet /usr/bin
cp /opt/kubernetes/server/bin/kube-proxy /usr/bin
cp /opt/kubernetes/server/bin/kube-scheduler /usr/bin
chmod +x /usr/bin/kube*
chmod +x /usr/bin/hyperkube
\end{code-in-enumerate}

  \1 添加kubernetes用户及相关路径
\begin{code-in-enumerate}{bash}
groupadd -r kube
useradd -r -g kube -d / -s /sbin/nologin -c "Kubernetes user" kube
mkdir -p /etc/kubernetes /var/run/kubernetes /var/lib/kube-dns /var/lib/kubelet
cat >/etc/kubernetes/config<<EOF
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR="--logtostderr=true"
# journal message level, 0 is debug
KUBE_LOG_LEVEL="--v=0"
# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV="--allow-privileged=true"
# How the controller-manager, scheduler, and proxy find the apiserver
KUBE_MASTER="--master=http://10.1.1.16:8080"
EOF
chown -R kube:kube /etc/kubernetes /var/run/kubernetes /var/lib/kube-dns /var/lib/kubelet
\end{code-in-enumerate}

\end{outline}

\section{Kubernetes安装}
根据服务器的角色不同，kubernets分为monitor和node。这2种服务器的安装方式有一些区别。

\subsection{Monitor的配置}
\begin{code-block}{bash}
cat >/etc/kubernetes/apiserver<<EOF
###
# kubernetes system config
#
# The following values are used to configure the kube-apiserver
#
# The address on the local server to listen to.
KUBE_API_ADDRESS="--insecure-bind-address=0.0.0.0"
# The port on the local server to listen on.
KUBE_API_PORT="--port=8080"
# Port minions listen on
KUBELET_PORT="--kubelet-port=10250"
# Comma separated list of nodes in the etcd cluster
KUBE_ETCD_SERVERS="--etcd-servers=http://10.1.1.16:2379,http://10.1.1.17:2379,http://10.1.1.18:2379"
# Address range to use for services
# service cluster ip一定不能和etcd的ip range冲突！！
KUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=108.36.0.0/16"
# default admission control policies
KUBE_ADMISSION_CONTROL=\
"--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota"
KUBE_API_ARGS="--client-ca-file=/etc/kubernetes/credentials/ca.crt \\
               --tls-private-key-file=/etc/kubernetes/credentials/server.key \\
               --tls-cert-file=/etc/kubernetes/credentials/server.crt"
EOF

cat >/etc/kubernetes/controller-manager<<EOF
KUBE_CONTROLLER_MANAGER_ARGS="--root-ca-file=/etc/kubernetes/credentials/ca.crt \\
    --service-account-private-key-file=/etc/kubernetes/credentials/server.key"
EOF

cat >/etc/kubernetes/scheduler<<EOF
KUBE_SCHEDULER_ARGS=""
EOF

cat >/etc/kubernetes/dns<<EOF
KUBE_DNS_PORT="--dns-port=53"
KUBE_DNS_DOMAIN="--domain=k8s.centos.me"
KUBE_DNS_MASTER="--kube-master-url=http://10.1.1.16:8080"
KUBE_DNS_ARGS=""
EOF

mkdir -p /etc/kubernetes/credentials
cd /etc/kubernetes/credentials
export MASTER_IP="10.1.1.16"
export MASTER_NAME="k8smon"
openssl genrsa -out ca.key 2048
openssl req -x509 -new -nodes -key ca.key -subj "/CN=${MASTER_IP}" -days 10000 -out ca.crt
openssl genrsa -out server.key 2048
openssl req -new -key server.key -subj "/CN=${MASTER_NAME}" -out server.csr
openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 10000

cat >/usr/lib/systemd/system/kube-apiserver.service<<EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
After=etcd.service
[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/apiserver
User=kube
ExecStart=/usr/bin/kube-apiserver \\
            \$KUBE_LOGTOSTDERR \\
            \$KUBE_LOG_LEVEL \\
            \$KUBE_ETCD_SERVERS \\
            \$KUBE_API_ADDRESS \\
            \$KUBE_API_PORT \\
            \$KUBELET_PORT \\
            \$KUBE_ALLOW_PRIV \\
            \$KUBE_SERVICE_ADDRESSES \\
            \$KUBE_ADMISSION_CONTROL \\
            \$KUBE_API_ARGS
Restart=on-failure
Type=notify
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
EOF

cat >/usr/lib/systemd/system/kube-controller-manager.service<<EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/controller-manager
User=kube
ExecStart=/usr/bin/kube-controller-manager \\
            \$KUBE_LOGTOSTDERR \\
            \$KUBE_LOG_LEVEL \\
            \$KUBE_MASTER \\
            \$KUBE_CONTROLLER_MANAGER_ARGS
Restart=on-failure
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
EOF

cat >/usr/lib/systemd/system/kube-scheduler.service<<EOF
[Unit]
Description=Kubernetes Scheduler Plugin
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/scheduler
User=kube
ExecStart=/usr/bin/kube-scheduler \\
            \$KUBE_LOGTOSTDERR \\
            \$KUBE_LOG_LEVEL \\
            \$KUBE_MASTER \\
            \$KUBE_SCHEDULER_ARGS
Restart=on-failure
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
EOF

cat >/usr/lib/systemd/system/kube-dns.service<<EOF
[Unit]
Description=Kubernetes Kube-dns Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=kube-apiserver.service
Requires=kube-apiserver.service
[Service]
WorkingDirectory=/var/lib/kube-dns
EnvironmentFile=-/etc/kubernetes/dns
ExecStart=/usr/bin/kube-dns \\
            \$KUBE_DNS_PORT \\
            \$KUBE_DNS_DOMAIN \\
            \$KUBE_DNS_MASTER \\
            \$KUBE_DNS_ARGS
Restart=on-failure
[Install]
WantedBy=multi-user.target
EOF

chown -R kube:kube /etc/kubernetes /var/run/kubernetes /var/lib/kube-dns /var/lib/kubelet

systemctl daemon-reload
for id in kube-{apiserver,scheduler,controller-manager,dns};\
    do systemctl enable $id;systemctl start $id;done
\end{code-block}

\subsection{Node的配置}
\begin{code-block}{bash}
cat >/etc/kubernetes/kubelet<<EOF
#### kubernetes kubelet (minion) config
# The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)
KUBELET_ADDRESS="--address=0.0.0.0"
# The port for the info server to serve on
KUBELET_PORT="--port=10250"
# You may leave this blank to use the actual hostname
KUBELET_HOSTNAME="--hostname-override="
# location of the api-server
KUBELET_API_SERVER="--api-servers=http://10.1.1.16:8080"
# pod infrastructure container
KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=10.1.1.16:5000/rhel7/pod-infrastructure:latest"
# Add your own!
KUBELET_ARGS="--cluster-dns=10.1.1.16 --cluster-domain=k8s.centos.me"
EOF

cat >/etc/kubernetes/proxy<<EOF
#### kubernetes proxy config
# default config should be adequate
# Add your own!
KUBE_PROXY_ARGS=""
EOF

cat >/usr/lib/systemd/system/kubelet.service<<EOF
[Unit]
Description=Kubernetes Kubelet Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service
[Service]
WorkingDirectory=/var/lib/kubelet
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/kubelet
ExecStart=/usr/bin/kubelet \\
            \$KUBE_LOGTOSTDERR \\
            \$KUBE_LOG_LEVEL \\
            \$KUBELET_API_SERVER \\
            \$KUBELET_ADDRESS \\
            \$KUBELET_PORT \\
            \$KUBELET_HOSTNAME \\
            \$KUBE_ALLOW_PRIV \\
            \$KUBELET_POD_INFRA_CONTAINER \\
            \$KUBELET_ARGS
Restart=on-failure
[Install]
WantedBy=multi-user.target
EOF

cat >/usr/lib/systemd/system/kube-proxy.service<<EOF
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/proxy
ExecStart=/usr/bin/kube-proxy \\
            \$KUBE_LOGTOSTDERR \\
            \$KUBE_LOG_LEVEL \\
            \$KUBE_MASTER \\
            \$KUBE_PROXY_ARGS
Restart=on-failure
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
EOF

chown -R kube:kube /etc/kubernetes /var/run/kubernetes /var/lib/kube-dns /var/lib/kubelet

systemctl daemon-reload
for id in {kubelet,kube-proxy};\
    do systemctl enable $id;systemctl start $id;done
\end{code-block}

\section{Kubernetes的基本使用}
\begin{code-block}{bash}
# 查看k8s集群的节点
[root@k8smon ~]# kubectl get nodes
NAME      STATUS    AGE
k8s1      Ready     1d
k8s2      Ready     1d
k8s3      Ready     1d

# 查看命名空间
[root@k8smon ~]# kubectl get namespace
NAME          STATUS    AGE
default       Active    1d
kube-system   Active    1d

# 如果没有kube-system，则需要新建
cat >kubu-system-ns.yaml<<EOF
apiVersion: v1
kind: Namespace
metadata:
  name: kube-system
EOF
kubectl create -f kube-system-ns.yaml
\end{code-block}

k8s的基础有了之后，可以进行进一步的操作，所使用的yaml文件如附件：\textattachfile{influxdb-grafana-rc.yaml}{\textcolor{blue}{influxdb-grafana-rc.yaml}}，
\textattachfile{grafana-svc.yaml}{\textcolor{blue}{grafana-svc.yaml}}，
\textattachfile{heapster-rc.yaml}{\textcolor{blue}{heapster-rc.yaml}}，
\textattachfile{heapster-svc.yaml}{\textcolor{blue}{heapster-svc.yaml}}，
\textattachfile{k8s-dashboard-rc.yaml}{\textcolor{blue}{k8s-dashboard-rc.yaml}}和
\textattachfile{k8s-dashboard-svc.yaml}{\textcolor{blue}{k8s-dashboard-svc.yaml}}，
\textattachfile{influxdb-svc.yaml}{\textcolor{blue}{influxdb-svc.yaml}}。
\label{heapster}
\begin{code-block}{bash}
# 创建 influxdb-grafana-rc
kubectl create -f influxdb-grafana-rc.yaml

# 创建 influxdb-svc 和 grafana-svc
kubectl create -f influxdb-svc.yaml
kubectl create -f grafana-svc.yaml

# 创建heapster-rc
kubectl create -f heapster-rc.yaml

# 创建 heapster-svc
kubectl create -f heapster-svc.yaml

# 创建 k8s-dashboard-rc
kubectl create -f k8s-dashboard-rc.yaml

# 创建k8s-dashboard-svc
kubectl create -f k8s-dashboard-svc.yaml
\end{code-block}

至此，kubernets的环境搭建完成。需要注意的是，heapster是用于kubernetes的监控，
使用kubernetes实现hpa（水平自动扩展）的时候，这是必须的组件，否则是不会生效的。

\section{Commands of Kubernetes}
\begin{code-block}{bash}
# 查询deployments
kubectl get --namespace kube-system deployments

# 查询单个的deployment
kubectl get --namespace kube-system deployments nginx-ingress-controller

# 显示单个的deployment
kubectl describe --namespace kube-system deployments nginx-ingress-controller

# 进入pods的container
kubectl exec -it pods --container container1 -- /bin/bash

\end{code-block}

\section{Usage of Kubernetes}
\begin{outline}[enumerate]
\1 创建namespace
\begin{code-in-enumerate}{bash}
cat >zhangjl-ns.yaml<<EOF
apiVersion: v1
kind: Namespace
metadata:
  name: zhangjl-ns
EOF
kubectl create -f zhangjl-ns.yaml
\end{code-in-enumerate}

\1 为namespace的资源设置限制
\begin{code-in-enumerate}{bash}
cat >zhangjl-ns-limits.yaml<<EOF
apiVersion: v1
kind: LimitRange
metadata:
  name: zhangjl-ns-limits
  namespace: zhangjl-ns
spec:
  limits:
  - max:
      cpu: "2"
      memory: 1Gi
    min:
      cpu: 200m
      memory: 6Mi
    type: Pod
  - default:
      cpu: 300m
      memory: 200Mi
    defaultRequest:
      cpu: 200m
      memory: 100Mi
    max:
      cpu: "2"
      memory: 1Gi
    min:
      cpu: 100m
      memory: 3Mi
    type: Container
EOF
kubectl create -f zhangjl-ns-limits.yaml
\end{code-in-enumerate}
但是需要注意，这个限制是作用在pod和container上的，并不是限制整个的namespace资源使用。
也就是说，如果在这个namespace当中创建pod，每个pod的最大cpu占用不能超过2颗，最大mem
占用不能超过1G。

\1 为namespace设置资源限制
\begin{code-in-enumerate}{bash}
cat >zhangjl-ns-quota.yaml<<EOF
apiVersion: v1
kind: ResourceQuota
metadata:
  name: zhangjl-ns-quota
  namespace: zhangjl-ns
spec:
  hard:
    cpu: "8"
    memory: 4Gi
    persistentvolumeclaims: "10"
    pods: "10"
    replicationcontrollers: "20"
    resourcequotas: "1"
    secrets: "10"
    services: "5"
EOF
kubectl create -f zhangjl-ns-quota.yaml
\end{code-in-enumerate}
这里的资源限制，才是针对整个namespace的限制。限制了namespace只能占用8个cpu，4G的内存，
在这个namespace当中，最多只能创建10个pod，10个secrets，5个services。

\1 创建自定义的secret
\begin{code-in-enumerate}{bash}
cat>zhangjl-secret.yaml<<EOF
apiVersion: v1
kind: Secret
metadata:
  name: zhangjl-secret
  namespace: zhangjl-ns
  annotations:
    kubernetes.io/service-account.name: zhangjl-user
type: kubernetes.io/service-account-token
EOF

kubectl create -f zhangjl-secret.yaml
\end{code-in-enumerate}

\1 创建自定义的用户
\begin{code-in-enumerate}{bash}
cat>zhangjl-user.yaml<<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: zhangjl-user
  namespace: zhangjl-ns
imagePullSecrets:
- name: zhangjl-secret
EOF

kubectl create -f zhangjl-secret.yaml
\end{code-in-enumerate}

\1 配置自定义的context
\begin{code-in-enumerate}{bash}
kubectl config set-context zhangjl-ctx --namespace zhangjl-ns --user zhangjl-user
\end{code-in-enumerate}

\1 切换自定义的context
\begin{code-in-enumerate}{bash}
kubectl config use-context zhangjl-ctx
\end{code-in-enumerate}

\1 退出自定义的context
\begin{code-in-enumerate}{bash}
kubectl config unset current-context
\end{code-in-enumerate}

\1 创建deployments
\begin{code-in-enumerate}{bash}
cat >zhangjl-ns-deployments<<EOF
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  generation: 1
  labels:
    run: nginx-1.10.3
    image: nginx-1.10.3
  name: zhangjl-nginx-deployment
  namespace: zhangjl-ns
spec:
  replicas: 1
  selector:
    matchLabels:
      run: nginx-1.10.3
      image: nginx-1.10.3
  template:
    metadata:
      labels:
        run: nginx-1.10.3
        image: nginx-1.10.3
    spec:
      containers:
      - image: 10.1.1.16:5000/library/nginx:1.10.3
        imagePullPolicy: IfNotPresent
        name: zhangjl-nginx-deployment
EOF

kubectl create -f zhangjl-ns-deployments.yaml
\end{code-in-enumerate}
Kubernetes的deployments和service比较类似。删除deployments的时候，并不会删除deployments
所创建的replication sets以及pods。创建deployments的时候，如果deployments需要的
replication sets和pods不存在，则会自动创建相关的资源；如果相关资源已经存在，则会自动关联，不会重新创建。
但是，kubernets的rc和rs被删除的时候，对应的资源是会被删除的。

\1 创建水平自动扩展

自动扩展要求有监控服务的存在，因此，\colorunderlineref{heapster}的安装是必不可少的。另外，自动扩展针对的对象
是deployments，并且，deployments当中的每个资源使用都有限制，才可以进行自动扩展和收缩。
由于我们已经限定了namespace的quota和limits，因此，可以不用添加其他参数。但是，如果
namespace当中没有设置limits，则需要在创建deployments时，需要指定参数--requests=cpu=xxx。
另外，目前，自动扩展的监控指标只能根据cpu的监控值，无法通过内存的使用进行自动扩展。
\begin{code-in-enumerate}{bash}
kubectl autoscale deployment zhangjl-nginx-deployment --cpu-percent=10 --min=1 --max=4

# 也可以通过yaml文件创建hpa
cat >zhangjl-ns-deployment-hpa<<EOF
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: zhangjl-nginx-deployment-hpa
  namespace: zhangjl-ns
spec:
  maxReplicas: 4
  minReplicas: 1
  scaleTargetRef:
    apiVersion: extensions/v1beta1
    kind: Deployment
    name: zhangjl-nginx-deployment
  targetCPUUtilizationPercentage: 10
EOF
kubectl create -f zhangjl-ns-deployment-hpa.yaml

\end{code-in-enumerate}
上面的命令表示创建了一个水平自动扩展（hap），即当pod的cpu占用超过10\%之后，开始进行弹性扩展，
但是，最多扩展到4个pod；扩展完毕之后，如果pod的cpu占用比开始下降，并且下降到10%以下，则开始
进行弹性收缩，最后只保留一个pod。

\1 滚动升级

目前，官网上的滚动升级只是针对rc的，暂时没有看到针对deployment或者rs的滚动升级。
滚动升级的方式分为2种：直接指定image以及通过yaml文件滚动升级。
\begin{code-in-enumerate}{bash}
# 直接指定image的方式
# nginx-rc-v1为可选的参数，表示滚动升级之后，rc的名称需要进行变化。
# 如果不需要变化名称，则无需指定nginx-rc-v1
kubekubectl rolling-update nginx-rc nginx-rc-v1 --image=10.1.1.16:5000/library/nginx:1.11.10

# 通过yaml文件进行滚动升级
cat >zhangjl-ns-rc-v2.yaml<<EOF
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx-rc-v2
  namespace: zhangjl-ns
  labels:
    app: nginx-rc-v2
    kubernetes.io/cluster-service: "true"
spec:
  replicas: 1
  selector:
    app: nginx-rc-v2
  template:
    metadata:
      labels:
        app: nginx-rc-v2
        kubernetes.io/cluster-service: "true"
    spec:
      containers:
      - name: nginx-rc-v2
        image: 10.1.1.16:5000/library/nginx:1.11.10
EOF

kubectl rolling-update nginx-rc -f zhangjl-ns-rc-v2.yaml
\end{code-in-enumerate}

\1 指定节点调度pod

Kubernetes目前支持指定节点调度pod。调度的依据是根据pod包含的container的nodeselector选择器
以及kubernetes node的label进行。
\begin{code-in-enumerate}{bash}
# 查看node的label
kubectl get nodes --show-labels
#NAME       STATUS    AGE       LABELS
#k8smon     Ready     3d        beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=k8smon
#k8snode1   Ready     3d        beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=k8snode1
#k8snode2   Ready     3d        beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=k8snode2

cat >zhangjl-ns-deployment-with-associated-node.yaml<<EOF
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  generation: 1
  labels:
    targetNode: k8snode2
  name: zhangjl-nginx-deployment-with-node-associated
  namespace: zhangjl-ns
spec:
  replicas: 1
  selector:
    matchLabels:
      targetNode: k8snode2
  template:
    metadata:
      labels:
        targetNode: k8snode2
    spec:
      containers:
      - image: 10.1.1.16:5000/library/nginx:1.10.3
        imagePullPolicy: IfNotPresent
        name: zhangjl-nginx-deployment-with-node-associated
      nodeSelector:
        kubernetes.io/hostname: k8snode2
EOF

kubectl create -f zhangjl-ns-deployment-with-associated-node.yaml
\end{code-in-enumerate}

\1 升级现有的deployments
\begin{code-in-enumerate}{bash}
cat >zhangjl-ns-deployments-update.yaml<<EOF
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  generation: 1
  labels:
    run: nginx-1.10.3
    image: nginx-1.10.3
  name: zhangjl-nginx-deployment
  namespace: zhangjl-ns
spec:
  replicas: 2 # 修改pod的数量
  selector:
    matchLabels:
      run: nginx-1.10.3
      image: nginx-1.10.3
  template:
    metadata:
      labels:
        run: nginx-1.10.3
        image: nginx-1.10.3
    spec:
      containers:
      - image: 20.30.40.8:5000/library/nginx:1.10.3 # 修改镜像版本
        imagePullPolicy: IfNotPresent
        name: zhangjl-nginx-deployment
EOF
# 使用replace和apply效果都是一样的。
# kubectl replace -f zhangjl-ns-deployments-update.yaml
kubectl apply -f zhangjl-ns-deployments-update.yaml
\end{code-in-enumerate}
Deployment的update有一个问题，目前看起来可能算一个bug：如果删除了原有的rs，则会
导致一些pod被删除。因此，升级deployment之后，最好不要删除以前的rs。

\1 Deployments的滚动升级和回滚
\begin{code-in-enumerate}{bash}
kubectl apply -f zhangjl-ns-deployments-update.yaml
# 查看升级的历史
kubectl rollout history deployment/zhangjl-nginx-deployment

# 查看单独一个升级的详细信息
kubectl rollout history deployment/zhangjl-nginx-deployment --revision=3

# 回退到指定的一个升级
kubectl rollout undo deployment/zhangjl-nginx-deployment --to-revision=2
\end{code-in-enumerate}
需要注意的是，deployment的滚动升级只针对变更image镜像，对于replica的变更，通常
采用的是scall out。

\1 使用ceph作为kubernetes的volume
\begin{code-in-enumerate}{bash}
cat >zhangjl-ns-ceph.yaml<<EOF
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  generation: 1
  labels:
    app: nginx-ceph
    image: nginx
    storage: ceph
  name: zhangjl-deployment-ceph
  namespace: zhangjl-ns
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-ceph
      image: nginx
      storage: ceph
  template:
    metadata:
      labels:
        app: nginx-ceph
        image: nginx
        storage: ceph
    spec:
      containers:
      - image: 20.30.40.8:5000/library/nginx:1.10.3
        imagePullPolicy: IfNotPresent
        name: zhangjl-deployment-ceph
        volumeMounts:
        - mountPath: /opt
          name: cinder-ceph
      volumes:
      - name: cinder-ceph
        rbd:
          monitors:
          - 30.40.50.7:6789
          - 30.40.50.4:6789
          - 30.40.50.14:6789
          pool: volumes
          image: volume-a6dd0f9f-9a35-43c8-9b7a-e773fbeea58a
          user: awcloud
          keyring: /etc/ceph/ceph.client.awcloud.keyring
          fsType: ext4
          readOnly: false
EOF

kubectl create -f zhangjl-ns-ceph.yaml

# 验证是否挂载了rbd的磁盘
mount | grep kub
\end{code-in-enumerate}
需要注意的是，如果使用ceph作为kubernetes的volume，则需要在kubernetes的主机上安装
ceph-mon，并且，配置文件和keyring文件需要和ceph节点的一致（/etc/ceph/）。

\1 导入ceph的secret到kubernetes
\begin{code-in-enumerate}{bash}
export ceph_secret=`grep key /etc/ceph/ceph.client.awcloud.keyring |awk '{printf "%s", $NF}'|base64`
cat >zhangjl-ns-ceph-secret.yaml<<EOF
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
  namespace: zhangjl-ns
type: "kubernetes.io/rbd"
data:
key: $ceph_secret
EOF
kubectl create -f zhangjl-ns-ceph-secret.yaml
\end{code-in-enumerate}
导入ceph的secret之后，使用在kubernetes当中使用ceph可以替换为下列的方式。
\begin{code-in-enumerate}{bash}
cat >zhangjl-ns-ceph-with-secret.yaml<<EOF
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  generation: 1
  labels:
    app: nginx-ceph
    image: nginx
    storage: ceph
  name: zhangjl-deployment-ceph
  namespace: zhangjl-ns
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-ceph
      image: nginx
      storage: ceph
  template:
    metadata:
      labels:
        app: nginx-ceph
        image: nginx
        storage: ceph
    spec:
      containers:
      - image: 20.30.40.8:5000/library/nginx:1.10.3
        imagePullPolicy: IfNotPresent
        name: zhangjl-deployment-ceph
        volumeMounts:
        - mountPath: /opt
          name: cinder-ceph
      volumes:
      - name: cinder-ceph
        rbd:
          monitors:
          - 30.40.50.7:6789
          - 30.40.50.4:6789
          - 30.40.50.14:6789
          pool: volumes
          image: volume-a6dd0f9f-9a35-43c8-9b7a-e773fbeea58a
          user: awcloud
          secretRef:
            name: ceph-secret
          fsType: ext4
          readOnly: false
EOF
kubectl create -f zhangjl-ns-ceph-with-secrete.yaml
\end{code-in-enumerate}

\end{outline}
